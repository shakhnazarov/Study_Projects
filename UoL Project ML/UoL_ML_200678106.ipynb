{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import kaleido\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_UL = pd.read_csv('Country-data_Unsupervised.csv')\n",
    "df_UL.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data description\n",
    "df_UL.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_UL.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr = df_UL.corr()\n",
    "# Fill diagonal and upper half with NaNs\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "corr[mask] = np.nan\n",
    "(corr\n",
    " .style\n",
    " .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)\n",
    " .highlight_null(color='#f1f1f1')  # Color NaNs grey\n",
    " .format(precision=2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# divide features into numerical and categorical\n",
    "cols = list(df_UL.columns)\n",
    "cols.remove('country')\n",
    "cat_feats = ['country']\n",
    "num_feats = [*cols]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot distribution of features\n",
    "fig, ax = plt.subplots(nrows = 3,ncols = 3,figsize = (15,15))\n",
    "for i in range(len(num_feats)):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    sns.distplot(df_UL[num_feats[i]],color = 'red')\n",
    "    title = 'Distribution : ' + num_feats[i]\n",
    "    plt.title(title)\n",
    "plt.show();"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# scale features\n",
    "df_scal = pd.DataFrame()\n",
    "df_scal['Health'] = (df_UL['child_mort'] / df_UL['child_mort'].mean()) + (df_UL['health'] / df_UL['health'].mean()) + (df_UL['life_expec'] / df_UL['life_expec'].mean()) + (df_UL['total_fer'] / df_UL['total_fer'].mean())\n",
    "df_scal['Trade'] = (df_UL['imports'] / df_UL['imports'].mean()) + (df_UL['exports'] / df_UL['exports'].mean())\n",
    "df_scal['Finance'] = (df_UL['income'] / df_UL['income'].mean()) + (df_UL['inflation'] / df_UL['inflation'].mean()) + (df_UL['gdpp'] / df_UL['gdpp'].mean())\n",
    "df_scal.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MinMax Scaling\n",
    "\n",
    "mms = MinMaxScaler() # Normalization\n",
    "ss = StandardScaler() # Standardization\n",
    "df_scal['Health'] = mms.fit_transform(df_scal[['Health']])\n",
    "df_scal['Trade'] = mms.fit_transform(df_scal[['Trade']])\n",
    "df_scal['Finance'] = mms.fit_transform(df_scal[['Finance']])\n",
    "df_scal.insert(loc = 0, value = list(df_UL['country']), column = 'Country')\n",
    "df_scal.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA\n",
    "df_PCA = df_UL.copy(deep = True)\n",
    "\n",
    "col = list(df_UL.columns)\n",
    "col.remove('health'); col.remove('country')\n",
    "\n",
    "df_PCA['health'] = ss.fit_transform(df_PCA[['health']]) # Standardization\n",
    "\n",
    "for i in col:\n",
    "    df_PCA[i] = mms.fit_transform(df_PCA[[i]]) # Normalization\n",
    "df_PCA.drop(columns = 'country',inplace = True)\n",
    "df_PCA.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA further\n",
    "\n",
    "pca = PCA()\n",
    "df_PCA_2 = pd.DataFrame(pca.fit_transform(df_PCA))\n",
    "pca.explained_variance_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot PCA\n",
    "plt.step(list(range(1,10)), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Eigen Values')\n",
    "plt.ylabel('Ratio of Variance Explained')\n",
    "plt.title('Variance Covered by each Eigen Value')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# combination features\n",
    "m1 = df_scal.drop(columns = ['Country']).values\n",
    "m2 = df_PCA_2.values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "sse = {};sil = [];kmax = 10\n",
    "fig = plt.subplots(nrows = 1, ncols = 2, figsize = (20,5))\n",
    "\n",
    "# Elbow Method :\n",
    "plt.subplot(1,2,1)\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(m2)\n",
    "    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n",
    "sns.lineplot(x = list(sse.keys()), y = list(sse.values()));\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel(\"k : Number of cluster\")\n",
    "plt.ylabel(\"Sum of Squared Error\")\n",
    "plt.grid()\n",
    "\n",
    "# Silhouette Score Method\n",
    "plt.subplot(1,2,2)\n",
    "for k in range(2, kmax + 1):\n",
    "    kmeans = KMeans(n_clusters = k).fit(m2)\n",
    "    labels = kmeans.labels_\n",
    "    sil.append(silhouette_score(m2, labels, metric = 'euclidean'))\n",
    "sns.lineplot(x = range(2,kmax + 1), y = sil);\n",
    "plt.title('Silhouette Score Method')\n",
    "plt.xlabel(\"k : Number of cluster\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = KMeans(n_clusters = 3,max_iter = 1000)\n",
    "model.fit(m2)\n",
    "cluster = model.cluster_centers_\n",
    "centroids = np.array(cluster)\n",
    "labels = model.labels_\n",
    "df_UL['Class'] = labels; df_PCA_2['Class'] = labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df_PCA_2.insert(0,column = 'Country', value = df_UL['country'])\n",
    "\n",
    "df_PCA_2['Class'].loc[df_PCA_2['Class'] == 0] = 'Might Need Help'\n",
    "df_PCA_2['Class'].loc[df_PCA_2['Class'] == 1] = 'Help Needed'\n",
    "df_PCA_2['Class'].loc[df_PCA_2['Class'] == 2] = 'No Help Needed'\n",
    "\n",
    "fig = px.choropleth(df_PCA_2[['Country','Class']],\n",
    "                    locationmode = 'country names',\n",
    "                    locations = 'Country',\n",
    "                    title = 'Needed Help Per Country (World)',\n",
    "                    color = df_PCA_2['Class'],\n",
    "                    color_discrete_map = {'Help Needed':'#d62728',\n",
    "                                          'Might Need Help':'#bcbd22',\n",
    "                                          'No Help Needed': '#1f77b4'})\n",
    "fig.update_geos(fitbounds = \"locations\", visible = True)\n",
    "fig.update_layout(legend_title_text = 'Labels',legend_title_side = 'top',title_pad_l = 260,title_y = 0.86)\n",
    "fig.show(engine = 'kaleido')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hierarchical clustering\n",
    "\n",
    "linkage_data = linkage(m2, method = 'ward', metric = 'euclidean')\n",
    "dendrogram(linkage_data)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "hierarchical_cluster = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\n",
    "labels = hierarchical_cluster.fit(m2)\n",
    "\n",
    "pred_agc = pd.Series(hierarchical_cluster.labels_)\n",
    "df_UL['Class'] = pred_agc; df_PCA_2['Class'] = pred_agc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_PCA_2['Class'].loc[df_PCA_2['Class'] == 0] = 'Help Needed'\n",
    "df_PCA_2['Class'].loc[df_PCA_2['Class'] == 1] = 'Might Need Help'\n",
    "df_PCA_2['Class'].loc[df_PCA_2['Class'] == 2] = 'No Help Needed'\n",
    "\n",
    "fig = px.choropleth(df_PCA_2[['Country','Class']],\n",
    "                    locationmode = 'country names',\n",
    "                    locations = 'Country',\n",
    "                    title = 'Needed Help Per Country (World)',\n",
    "                    color = df_PCA_2['Class'],\n",
    "                    color_discrete_map={'Help Needed':'Red',\n",
    "                                        'Might Need Help':'Yellow',\n",
    "                                        'No Help Needed':'Green'})\n",
    "fig.update_geos(fitbounds = \"locations\", visible = True)\n",
    "fig.update_layout(legend_title_text = 'Labels',legend_title_side = 'top',title_pad_l = 260,title_y = 0.86)\n",
    "fig.show(engine = 'kaleido')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_reg = pd.read_csv('auto-mpg-regression.csv', na_values = \"?\")\n",
    "df_reg.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_reg.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def auto_preprocess(dataframe):\n",
    "    df_ = dataframe.copy()\n",
    "    auto_misspelled = {'chevroelt': 'chevrolet',\n",
    "                       'chevy': 'chevrolet',\n",
    "                       'vokswagen': 'volkswagen',\n",
    "                       'vw': 'volkswagen',\n",
    "                       'hi': 'harvester',\n",
    "                       'maxda': 'mazda',\n",
    "                       'toyouta': 'toyota',\n",
    "                       'mercedes-benz': 'mercedes'}\n",
    "    df_['make'] = [auto_misspelled[key].title() if key in auto_misspelled else\n",
    "                   key.title() for key in [i.split()[0] for i in df_['car name']]]\n",
    "    df_['name'] = [' '.join(i.split()[1:]).title() for i in df_['car name']]\n",
    "\n",
    "    df_ = df_.drop(columns = ['car name'], axis = 1)\n",
    "    return df_\n",
    "\n",
    "df_reg = auto_preprocess(df_reg)\n",
    "df_reg.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# types of variables\n",
    "\n",
    "def check_class(dataframe):\n",
    "    nunique_df = pd.DataFrame({'Variable': dataframe.columns,\n",
    "                               'Classes': [dataframe[i].nunique() \\\n",
    "                                           for i in dataframe.columns]})\n",
    "\n",
    "    nunique_df = nunique_df.sort_values('Classes', ascending=False)\n",
    "    nunique_df = nunique_df.reset_index(drop = True)\n",
    "    return nunique_df\n",
    "\n",
    "check_class(df_reg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr = df_reg.corr()\n",
    "# Fill diagonal and upper half with NaNs\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "corr[mask] = np.nan\n",
    "(corr\n",
    " .style\n",
    " .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)\n",
    " .highlight_null(color='#f1f1f1')  # Color NaNs grey\n",
    " .format(precision=2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get rid of missing\n",
    "cat_cols = ['cylinders', 'origin']\n",
    "df_reg['horsepower'] = df_reg['horsepower'].fillna(df_reg.groupby(cat_cols)['horsepower'].transform('median'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mpg explore\n",
    "\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "stats.probplot(df_reg[\"mpg\"], plot = plt)\n",
    "plt.title(\"Before Log1p Transformation\", size = 12)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#log`p\n",
    "df_reg[\"mpg\"] = np.log1p(df_reg[\"mpg\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# encoding\n",
    "\n",
    "def one_hot_encoder(dataframe, categorical_cols: list, drop_first: bool = False):\n",
    "    dataframe = pd.get_dummies(dataframe,\n",
    "                               columns = categorical_cols,\n",
    "                               drop_first = drop_first)\n",
    "    return dataframe\n",
    "\n",
    "def label_encoder(dataframe, binary_col):\n",
    "    labelencoder = LabelEncoder()\n",
    "    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n",
    "    return dataframe\n",
    "\n",
    "binary_cols = [col for col in df_reg.columns if df_reg[col].dtype not in [int, float]\n",
    "               and df_reg[col].nunique() == 2]\n",
    "print('Binary Features: {}'.format(binary_cols))\n",
    "\n",
    "ohe_cols = [col for col in df_reg.columns if 10 >= df_reg[col].nunique() > 2]\n",
    "ohe_cols.append('make')\n",
    "print('Multiclass Features: {}'.format(ohe_cols))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_reg['cylinders'] = df_reg['cylinders'].astype(int)\n",
    "df_reg['origin'] = df_reg['origin'].astype(int)\n",
    "df_reg = one_hot_encoder(df_reg, ohe_cols)\n",
    "df_reg.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "useless_cols = useless_cols = [col for col in df_reg.columns if df_reg[col].nunique() == 2 and\n",
    "                (df_reg[col].value_counts() / len(df_reg) < 0.03).any(axis=None)]\n",
    "\n",
    "print('Number of useless variables: {}'.format(len(useless_cols)))\n",
    "df_reg.drop(useless_cols, axis = 1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#split\n",
    "X = df_reg.drop(columns = [\"mpg\", 'name'], axis = 1)\n",
    "\n",
    "y = df_reg['mpg']\n",
    "\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "test_size = 0.2\n",
    "\n",
    "random_state = 154\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size  = test_size,\n",
    "                                                    random_state = random_state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train\n",
    "def train_pipeline(pipe):\n",
    "    result = dict()\n",
    "    scaler = pipe.steps[0][1].__class__.__name__\n",
    "    regressor = pipe.steps[1][1].__class__.__name__\n",
    "    result['model'] = regressor\n",
    "    result['scaler'] = scaler if scaler != 'NoneType' else 'Without Scaling'\n",
    "\n",
    "    #Training Model\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    #Get Predictions\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_test_exp = np.expm1(y_test)\n",
    "    y_pred_exp = np.expm1(y_pred)\n",
    "\n",
    "    #Model Evaluation\n",
    "    result['r2'] =  r2_score(y_test_exp, y_pred_exp),\n",
    "    result['mse'] =  mean_squared_error(y_test_exp, y_pred_exp)\n",
    "    return result\n",
    "\n",
    "scalers = [None, RobustScaler(), MinMaxScaler(), StandardScaler()]\n",
    "\n",
    "regressors = [LinearRegression(),\n",
    "              Lasso(), Ridge(),\n",
    "              RandomForestRegressor(), DecisionTreeRegressor(), GradientBoostingRegressor()]\n",
    "\n",
    "eval_data = pd.DataFrame()\n",
    "for reg in regressors:\n",
    "    for sc in scalers:\n",
    "        pipeline = Pipeline([('scaler', sc), ('reg', reg)])\n",
    "        eval_data = eval_data.append(pd.DataFrame(train_pipeline(pipeline)))\n",
    "    eval_data = eval_data.reset_index(drop = True)\n",
    "eval_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_data.transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Supervised task\n",
    "df_SL = pd.read_csv('heart supervised.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_SL.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\n",
    "con_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n",
    "target_col = [\"output\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_SL.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr = df_SL.corr()\n",
    "# Fill diagonal and upper half with NaNs\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "corr[mask] = np.nan\n",
    "(corr\n",
    " .style\n",
    " .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)\n",
    " .highlight_null(color='#f1f1f1')  # Color NaNs grey\n",
    " .format(precision=2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creating a copy of df\n",
    "df1 = df_SL\n",
    "\n",
    "# define the columns to be encoded and scaled\n",
    "cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\n",
    "con_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n",
    "\n",
    "# encoding the categorical columns\n",
    "df1 = pd.get_dummies(df1, columns = cat_cols, drop_first = True)\n",
    "\n",
    "# defining the features and target\n",
    "X = df1.drop(['output'],axis=1)\n",
    "y = df1[['output']]\n",
    "\n",
    "# instantiating the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# scaling the continuous featuree\n",
    "X[con_cols] = scaler.fit_transform(X[con_cols])\n",
    "print(\"The first 5 rows of X are\")\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 154)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_supervised = {}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# instantiating the object and fitting\n",
    "clf = SVC(kernel='linear', C=1, random_state=154).fit(X_train,y_train)\n",
    "\n",
    "# predicting the values\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "models_supervised['SVM'] = f1_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# instantiating the object\n",
    "svm = SVC()\n",
    "\n",
    "# setting a grid - not so extensive\n",
    "parameters = {\"C\":np.arange(1,10,1),'gamma':[0.00001,0.00005, 0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1,5]}\n",
    "\n",
    "# instantiating the GridSearchCV object\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "\n",
    "# fitting the object\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# predicting the values\n",
    "y_pred = searcher.predict(X_test)\n",
    "\n",
    "models_supervised['SVM (tuned)'] = f1_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#logistic\n",
    "# instantiating the object\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fitting the object\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# calculating the probabilities\n",
    "y_pred_proba = logreg.predict_proba(X_test)\n",
    "\n",
    "# finding the predicted valued\n",
    "y_pred = np.argmax(y_pred_proba,axis=1)\n",
    "\n",
    "models_supervised['Logistic Regression'] = f1_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculating the probabilities\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# instantiating the roc_cruve\n",
    "fpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n",
    "\n",
    "# plotting the curve\n",
    "plt.plot([0,1],[0,1],\"k--\",'r+')\n",
    "plt.plot(fpr,tpr,label='Logistic Regression')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Logistric Regression ROC Curve\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# instantiating the object\n",
    "dt = DecisionTreeClassifier(random_state = 154)\n",
    "\n",
    "# fitting the model\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# calculating the predictions\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "models_supervised['Decision Tree'] = f1_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# instantiating the object\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# fitting the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# calculating the predictions\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "\n",
    "models_supervised['Random Forest'] = f1_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# instantiate the classifier\n",
    "gbt = GradientBoostingClassifier(n_estimators = 300,max_depth=1,subsample=0.8,max_features=0.2,random_state=154)\n",
    "\n",
    "# fitting the model\n",
    "gbt.fit(X_train,y_train)\n",
    "\n",
    "# predicting values\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "models_supervised['Gradient Boosting'] = f1_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sup_mods = pd.DataFrame(models_supervised, index=['F1'])\n",
    "df_sup_mods.transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
