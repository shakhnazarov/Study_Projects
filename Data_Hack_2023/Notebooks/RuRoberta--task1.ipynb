{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa245c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "# Defualt size of the plots\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8)\n",
    "#\n",
    "%matplotlib inline  \n",
    "\n",
    "# feature preprocessing \n",
    "import category_encoders as ce\n",
    "import nltk \n",
    "# IMPORT SOME MORE TEXT PREPROCESSORS \n",
    "\n",
    "# images \n",
    "import PIL\n",
    "import skimage\n",
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "# classic ML\n",
    "import sklearn as sk \n",
    "import catboost as cat\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers \n",
    "import lightning.pytorch as pl\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "\n",
    "# CV\n",
    "# from torchvision import transforms\n",
    "\n",
    "# parallelization\n",
    "import joblib\n",
    "\n",
    "# my lib\n",
    "import pqlib.utils as pq\n",
    "from pqlib.utils import get_datetime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33cbfcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq.get_datetime_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6306b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def METRIC(y_true, model_output):\n",
    "    model_output = np.array(model_output)\n",
    "    # removing nan class column\n",
    "    preds_probas = np.delete(model_output.reshape(-1,4).squeeze(), 0, axis=1)\n",
    "    # softmax for preds\n",
    "    preds_probas = scipy.special.softmax(preds_probas, axis=1)\n",
    "    \n",
    "    return roc_auc_score(y_true, preds_probas, multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24044869",
   "metadata": {},
   "source": [
    "#  Medium tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267da4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2592c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = 'bbc-text.csv'\n",
    "# df = pd.read_csv(datapath)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9205df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>2category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>При этом всегда получал качественные услуги.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>Не вижу, за что хотя бы 2 поставить, сервис на 1!</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>Вот так \"Мой любимый\" банк МКБ меня обманул.</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11640</th>\n",
       "      <td>Отвратительное отношение к клиентам.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>Всегда в любое время дня и ночи помогут, ответ...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence      1category  \\\n",
       "4754        При этом всегда получал качественные услуги.  Communication   \n",
       "4417   Не вижу, за что хотя бы 2 поставить, сервис на 1!              ?   \n",
       "3629        Вот так \"Мой любимый\" банк МКБ меня обманул.              ?   \n",
       "11640               Отвратительное отношение к клиентам.  Communication   \n",
       "5571   Всегда в любое время дня и ночи помогут, ответ...  Communication   \n",
       "\n",
       "      2category sentiment  \n",
       "4754        NaN         +  \n",
       "4417        NaN         −  \n",
       "3629        NaN         −  \n",
       "11640       NaN         −  \n",
       "5571        NaN         +  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = 'train.csv'\n",
    "df = pd.read_csv(datapath, index_col=0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81ce349",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'sentiment'\n",
    "TEXT = 'sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a64d8f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of a sequence\n",
      "1155\n"
     ]
    }
   ],
   "source": [
    "# Max length of a sequence\n",
    "lengths_text = df[TEXT].apply(len).values\n",
    "max_length = max(lengths_text)\n",
    "print(\"Max length of a sequence\")\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f35a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>2category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15839</th>\n",
       "      <td>4) КОНТРОЛЬ ЗА ОПЕРАЦИЯМИ - при попытки оплати...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17808</th>\n",
       "      <td>4) КОНТРОЛЬ ЗА ОПЕРАЦИЯМИ - при попытки оплати...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18935</th>\n",
       "      <td>4) КОНТРОЛЬ ЗА ОПЕРАЦИЯМИ - при попытки оплати...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>Communication</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence 1category  \\\n",
       "15839  4) КОНТРОЛЬ ЗА ОПЕРАЦИЯМИ - при попытки оплати...   Quality   \n",
       "17808  4) КОНТРОЛЬ ЗА ОПЕРАЦИЯМИ - при попытки оплати...   Quality   \n",
       "18935  4) КОНТРОЛЬ ЗА ОПЕРАЦИЯМИ - при попытки оплати...   Quality   \n",
       "\n",
       "           2category sentiment  \n",
       "15839            NaN         −  \n",
       "17808            NaN         −  \n",
       "18935  Communication         −  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[TEXT].apply(len) == max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120032a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4) КОНТРОЛЬ ЗА ОПЕРАЦИЯМИ - при попытки оплатить товар через интернет, карту временно заблокировали позвонили с банка узнать, что за операции Я пытаюсь совершить/ после уточнении данных по карте, паспорту и ключевому слову - карту разблокировали/ после данного инцидента желание совершать операции через интернет пропали\\xa05) СЛОЖНО ЗАБРАТЬ СВОИ ДЕНЬГИ - спустя 3 месяца пользования, карта перестала работать совсем и крайне сильно подвела Меня, когда Я находился в отпуске, доставив массу трудностей в связи ограниченной наличностью/ терпеть подобный «сервис» Я устал и решил сегодня закрыть счет с вместе с картой/ в итоге в отделении банке сказали, что, чтобы снять всю наличность через кассу, Мне нужна работающая карта, так как без Нее Мне не могут отдать сумму и в этом случае нужен ее перевыпуск (который естественно платный) или же Они могу сделать перевод на счет в другой банк, что Мне совершенно не нужно и нет такой такой возможности... ИТОГ: с таким ужасным сервисом и отношение к клиенту столкнулся впервые и очень разочарован услугами данного ОТП БАНКА/ Тому Кто захочет завести там карту, пусть хорошо подумает, нужен ли Вам такой «геморрой»'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[15839].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa12c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lengths_text,bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded0f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 19361 entries, 4754 to 8433\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentence   19361 non-null  object\n",
      " 1   1category  19361 non-null  object\n",
      " 2   2category  999 non-null    object\n",
      " 3   sentiment  19361 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "574cb81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence         0\n",
       "1category        0\n",
       "2category    18362\n",
       "sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f236e338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6464"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1d45733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 1category ##########\n",
      "Communication    7193\n",
      "?                5926\n",
      "Quality          5630\n",
      "Price             366\n",
      "Safety            246\n",
      "Name: 1category, dtype: int64\n",
      "\n",
      "########## 2category ##########\n",
      "Communication    474\n",
      "Quality          356\n",
      "Price            114\n",
      "Safety            55\n",
      "Name: 2category, dtype: int64\n",
      "\n",
      "########## sentiment ##########\n",
      "−    10192\n",
      "+     6262\n",
      "?     2907\n",
      "Name: sentiment, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col_ in df.columns.drop(TEXT):\n",
    "    print(f'########## {col_} ##########')\n",
    "    print(df[col_].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8be75277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates - all cols - 6464\n",
      "########## sentence ##########\n",
      "12195\n",
      "\n",
      "########## 1category ##########\n",
      "19356\n",
      "\n",
      "########## 2category ##########\n",
      "19356\n",
      "\n",
      "########## sentiment ##########\n",
      "19358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Duplicates - all cols - {df.duplicated().sum()}\")\n",
    "\n",
    "for col_ in df.columns:\n",
    "    print(f'########## {col_} ##########')\n",
    "    print(df[col_].duplicated().sum())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad8024f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>2category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13334</th>\n",
       "      <td>10.12.2020 20:57 Хотелось бы выразить огромну...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14601</th>\n",
       "      <td>10.12.2020 20:57 Хотелось бы выразить огромну...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15048</th>\n",
       "      <td>10.12.2020 20:57 Хотелось бы выразить огромну...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>!, тем самым оставив меня без средств к сущест...</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3926</th>\n",
       "      <td>!, тем самым оставив меня без средств к сущест...</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7241</th>\n",
       "      <td>– восхищаюсь я. Проходит еще неделя... Ну, я д...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7214</th>\n",
       "      <td>– восхищаюсь я. Проходит еще неделя... Ну, я д...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20234</th>\n",
       "      <td>№ ДО 53/Ф20-04-07/1333 от 19.09.2018 и также в...</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20075</th>\n",
       "      <td>№ ДО 53/Ф20-04-07/1333 от 19.09.2018 и также в...</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>№ ДО 53/Ф20-04-07/1333 от 19.09.2018 и также в...</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11470 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence      1category  \\\n",
       "13334   10.12.2020 20:57 Хотелось бы выразить огромну...        Quality   \n",
       "14601   10.12.2020 20:57 Хотелось бы выразить огромну...        Quality   \n",
       "15048   10.12.2020 20:57 Хотелось бы выразить огромну...        Quality   \n",
       "4481   !, тем самым оставив меня без средств к сущест...              ?   \n",
       "3926   !, тем самым оставив меня без средств к сущест...              ?   \n",
       "...                                                  ...            ...   \n",
       "7241   – восхищаюсь я. Проходит еще неделя... Ну, я д...  Communication   \n",
       "7214   – восхищаюсь я. Проходит еще неделя... Ну, я д...  Communication   \n",
       "20234  № ДО 53/Ф20-04-07/1333 от 19.09.2018 и также в...              ?   \n",
       "20075  № ДО 53/Ф20-04-07/1333 от 19.09.2018 и также в...              ?   \n",
       "981    № ДО 53/Ф20-04-07/1333 от 19.09.2018 и также в...              ?   \n",
       "\n",
       "      2category sentiment  \n",
       "13334       NaN         +  \n",
       "14601       NaN         +  \n",
       "15048       NaN         +  \n",
       "4481        NaN         −  \n",
       "3926        NaN         −  \n",
       "...         ...       ...  \n",
       "7241        NaN         ?  \n",
       "7214        NaN         ?  \n",
       "20234       NaN         ?  \n",
       "20075       NaN         ?  \n",
       "981         NaN         ?  \n",
       "\n",
       "[11470 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(keep=False)].sort_values(by=df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0502401",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c89ba8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OrdinalEncoder(cols=[&#x27;sentiment&#x27;],\n",
       "               mapping=[{&#x27;col&#x27;: &#x27;sentiment&#x27;, &#x27;data_type&#x27;: dtype(&#x27;O&#x27;),\n",
       "                         &#x27;mapping&#x27;: +      1\n",
       "−      2\n",
       "?      3\n",
       "NaN   -2\n",
       "dtype: int64}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OrdinalEncoder</label><div class=\"sk-toggleable__content\"><pre>OrdinalEncoder(cols=[&#x27;sentiment&#x27;],\n",
       "               mapping=[{&#x27;col&#x27;: &#x27;sentiment&#x27;, &#x27;data_type&#x27;: dtype(&#x27;O&#x27;),\n",
       "                         &#x27;mapping&#x27;: +      1\n",
       "−      2\n",
       "?      3\n",
       "NaN   -2\n",
       "dtype: int64}])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OrdinalEncoder(cols=['sentiment'],\n",
       "               mapping=[{'col': 'sentiment', 'data_type': dtype('O'),\n",
       "                         'mapping': +      1\n",
       "−      2\n",
       "?      3\n",
       "NaN   -2\n",
       "dtype: int64}])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_enc_cols_ = [TARGET]\n",
    "ord_enc = ce.OrdinalEncoder(\n",
    "    cols=ord_enc_cols_,\n",
    ")\n",
    "ord_enc.fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "637a69fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>2category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>При этом всегда получал качественные услуги.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>Не вижу, за что хотя бы 2 поставить, сервис на 1!</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>Вот так \"Мой любимый\" банк МКБ меня обманул.</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11640</th>\n",
       "      <td>Отвратительное отношение к клиентам.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>Всегда в любое время дня и ночи помогут, ответ...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>Деньги за страховой полис не вернули до сих по...</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13273</th>\n",
       "      <td>Хотелось бы так же прояснить, что до сложившей...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19523</th>\n",
       "      <td>\"РОСГОССТРАХ - БАНК\" - звучит круто и заманчиво!</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8004</th>\n",
       "      <td>Никогда и ни в коем случае не открывайте счет ...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8433</th>\n",
       "      <td>Данная ситуация меня сильно выбила из колеи, и...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12897 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence      1category  \\\n",
       "4754        При этом всегда получал качественные услуги.  Communication   \n",
       "4417   Не вижу, за что хотя бы 2 поставить, сервис на 1!              ?   \n",
       "3629        Вот так \"Мой любимый\" банк МКБ меня обманул.              ?   \n",
       "11640               Отвратительное отношение к клиентам.  Communication   \n",
       "5571   Всегда в любое время дня и ночи помогут, ответ...  Communication   \n",
       "...                                                  ...            ...   \n",
       "3238   Деньги за страховой полис не вернули до сих по...              ?   \n",
       "13273  Хотелось бы так же прояснить, что до сложившей...        Quality   \n",
       "19523   \"РОСГОССТРАХ - БАНК\" - звучит круто и заманчиво!              ?   \n",
       "8004   Никогда и ни в коем случае не открывайте счет ...  Communication   \n",
       "8433   Данная ситуация меня сильно выбила из колеи, и...  Communication   \n",
       "\n",
       "      2category  sentiment  \n",
       "4754        NaN          1  \n",
       "4417        NaN          2  \n",
       "3629        NaN          2  \n",
       "11640       NaN          2  \n",
       "5571        NaN          1  \n",
       "...         ...        ...  \n",
       "3238        NaN          2  \n",
       "13273       NaN          1  \n",
       "19523       NaN          1  \n",
       "8004        NaN          2  \n",
       "8433        NaN          2  \n",
       "\n",
       "[12897 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_enc.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ab347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37c2b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a mapping of the TARGET variable\n",
    "labels = ord_enc.mapping[0]['mapping'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39a53710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/ruRoberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "296ae07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  7476,   821,   998,   332, 36666,    18,     2,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "example_text = 'Мне будет тебя не хватать.'\n",
    "model_input = tokenizer(example_text,padding='max_length', max_length = 14, \n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "print(model_input['input_ids'])\n",
    "# print(model_input['token_type_ids'])\n",
    "print(model_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c063db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Мне будет тебя не хватать.</s><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "example_text = tokenizer.decode(model_input.input_ids[0])\n",
    "\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb4fc50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/ruRoberta-large')\n",
    "labels = labels\n",
    "\n",
    "class DatasetText(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in tqdm(df[TARGET], desc='Labels')]\n",
    "        self.texts = [tokenizer(text, \n",
    "                                padding='max_length', \n",
    "                                max_length = 512, \n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\"\n",
    "                               ) \n",
    "                      for text in tqdm(df[TEXT], desc='Tokens')\n",
    "                     ]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca258fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12897, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bfb89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10317 1290 1290\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2729cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# q = DatasetText(df)\n",
    "# q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01f1fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel, XLMRobertaForSequenceClassification, AutoModelForSequenceClassification, AutoModel\n",
    "\n",
    "class SeqClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_out_classes, dropout=0.5):\n",
    "\n",
    "        super(SeqClassifier, self).__init__()\n",
    "\n",
    "#         self.backbone = AutoModel.from_pretrained('xlm-roberta-base')\n",
    "        self.backbone = AutoModel.from_pretrained('sberbank-ai/ruRoberta-large')\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "#         self.linear = nn.Linear(768, num_out_classes) # XLM\n",
    "        self.linear = nn.Linear(1024, num_out_classes) # ruRoberta-large\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        # CHECK - out, out dimensions\n",
    "#         out = self.backbone(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "#         print(out)\n",
    "#         [print(out_.shape) for out_ in out]\n",
    "        \n",
    "        # pooled output - from the CLS token\n",
    "        # output from every \n",
    "        _, pooled_output = self.backbone(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        \n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f1647f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SeqClassifier(num_out_classes=len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "823c6492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True backbone.embeddings.word_embeddings.weight\n",
      "True backbone.embeddings.position_embeddings.weight\n",
      "True backbone.embeddings.token_type_embeddings.weight\n",
      "True backbone.embeddings.LayerNorm.weight\n",
      "True backbone.embeddings.LayerNorm.bias\n",
      "True backbone.encoder.layer.0.attention.self.query.weight\n",
      "True backbone.encoder.layer.0.attention.self.query.bias\n",
      "True backbone.encoder.layer.0.attention.self.key.weight\n",
      "True backbone.encoder.layer.0.attention.self.key.bias\n",
      "True backbone.encoder.layer.0.attention.self.value.weight\n",
      "True backbone.encoder.layer.0.attention.self.value.bias\n",
      "True backbone.encoder.layer.0.attention.output.dense.weight\n",
      "True backbone.encoder.layer.0.attention.output.dense.bias\n",
      "True backbone.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.0.intermediate.dense.weight\n",
      "True backbone.encoder.layer.0.intermediate.dense.bias\n",
      "True backbone.encoder.layer.0.output.dense.weight\n",
      "True backbone.encoder.layer.0.output.dense.bias\n",
      "True backbone.encoder.layer.0.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.0.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.1.attention.self.query.weight\n",
      "True backbone.encoder.layer.1.attention.self.query.bias\n",
      "True backbone.encoder.layer.1.attention.self.key.weight\n",
      "True backbone.encoder.layer.1.attention.self.key.bias\n",
      "True backbone.encoder.layer.1.attention.self.value.weight\n",
      "True backbone.encoder.layer.1.attention.self.value.bias\n",
      "True backbone.encoder.layer.1.attention.output.dense.weight\n",
      "True backbone.encoder.layer.1.attention.output.dense.bias\n",
      "True backbone.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.1.intermediate.dense.weight\n",
      "True backbone.encoder.layer.1.intermediate.dense.bias\n",
      "True backbone.encoder.layer.1.output.dense.weight\n",
      "True backbone.encoder.layer.1.output.dense.bias\n",
      "True backbone.encoder.layer.1.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.1.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.2.attention.self.query.weight\n",
      "True backbone.encoder.layer.2.attention.self.query.bias\n",
      "True backbone.encoder.layer.2.attention.self.key.weight\n",
      "True backbone.encoder.layer.2.attention.self.key.bias\n",
      "True backbone.encoder.layer.2.attention.self.value.weight\n",
      "True backbone.encoder.layer.2.attention.self.value.bias\n",
      "True backbone.encoder.layer.2.attention.output.dense.weight\n",
      "True backbone.encoder.layer.2.attention.output.dense.bias\n",
      "True backbone.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.2.intermediate.dense.weight\n",
      "True backbone.encoder.layer.2.intermediate.dense.bias\n",
      "True backbone.encoder.layer.2.output.dense.weight\n",
      "True backbone.encoder.layer.2.output.dense.bias\n",
      "True backbone.encoder.layer.2.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.2.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.3.attention.self.query.weight\n",
      "True backbone.encoder.layer.3.attention.self.query.bias\n",
      "True backbone.encoder.layer.3.attention.self.key.weight\n",
      "True backbone.encoder.layer.3.attention.self.key.bias\n",
      "True backbone.encoder.layer.3.attention.self.value.weight\n",
      "True backbone.encoder.layer.3.attention.self.value.bias\n",
      "True backbone.encoder.layer.3.attention.output.dense.weight\n",
      "True backbone.encoder.layer.3.attention.output.dense.bias\n",
      "True backbone.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.3.intermediate.dense.weight\n",
      "True backbone.encoder.layer.3.intermediate.dense.bias\n",
      "True backbone.encoder.layer.3.output.dense.weight\n",
      "True backbone.encoder.layer.3.output.dense.bias\n",
      "True backbone.encoder.layer.3.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.3.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.4.attention.self.query.weight\n",
      "True backbone.encoder.layer.4.attention.self.query.bias\n",
      "True backbone.encoder.layer.4.attention.self.key.weight\n",
      "True backbone.encoder.layer.4.attention.self.key.bias\n",
      "True backbone.encoder.layer.4.attention.self.value.weight\n",
      "True backbone.encoder.layer.4.attention.self.value.bias\n",
      "True backbone.encoder.layer.4.attention.output.dense.weight\n",
      "True backbone.encoder.layer.4.attention.output.dense.bias\n",
      "True backbone.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.4.intermediate.dense.weight\n",
      "True backbone.encoder.layer.4.intermediate.dense.bias\n",
      "True backbone.encoder.layer.4.output.dense.weight\n",
      "True backbone.encoder.layer.4.output.dense.bias\n",
      "True backbone.encoder.layer.4.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.4.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.5.attention.self.query.weight\n",
      "True backbone.encoder.layer.5.attention.self.query.bias\n",
      "True backbone.encoder.layer.5.attention.self.key.weight\n",
      "True backbone.encoder.layer.5.attention.self.key.bias\n",
      "True backbone.encoder.layer.5.attention.self.value.weight\n",
      "True backbone.encoder.layer.5.attention.self.value.bias\n",
      "True backbone.encoder.layer.5.attention.output.dense.weight\n",
      "True backbone.encoder.layer.5.attention.output.dense.bias\n",
      "True backbone.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.5.intermediate.dense.weight\n",
      "True backbone.encoder.layer.5.intermediate.dense.bias\n",
      "True backbone.encoder.layer.5.output.dense.weight\n",
      "True backbone.encoder.layer.5.output.dense.bias\n",
      "True backbone.encoder.layer.5.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.5.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.6.attention.self.query.weight\n",
      "True backbone.encoder.layer.6.attention.self.query.bias\n",
      "True backbone.encoder.layer.6.attention.self.key.weight\n",
      "True backbone.encoder.layer.6.attention.self.key.bias\n",
      "True backbone.encoder.layer.6.attention.self.value.weight\n",
      "True backbone.encoder.layer.6.attention.self.value.bias\n",
      "True backbone.encoder.layer.6.attention.output.dense.weight\n",
      "True backbone.encoder.layer.6.attention.output.dense.bias\n",
      "True backbone.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.6.intermediate.dense.weight\n",
      "True backbone.encoder.layer.6.intermediate.dense.bias\n",
      "True backbone.encoder.layer.6.output.dense.weight\n",
      "True backbone.encoder.layer.6.output.dense.bias\n",
      "True backbone.encoder.layer.6.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.6.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.7.attention.self.query.weight\n",
      "True backbone.encoder.layer.7.attention.self.query.bias\n",
      "True backbone.encoder.layer.7.attention.self.key.weight\n",
      "True backbone.encoder.layer.7.attention.self.key.bias\n",
      "True backbone.encoder.layer.7.attention.self.value.weight\n",
      "True backbone.encoder.layer.7.attention.self.value.bias\n",
      "True backbone.encoder.layer.7.attention.output.dense.weight\n",
      "True backbone.encoder.layer.7.attention.output.dense.bias\n",
      "True backbone.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.7.intermediate.dense.weight\n",
      "True backbone.encoder.layer.7.intermediate.dense.bias\n",
      "True backbone.encoder.layer.7.output.dense.weight\n",
      "True backbone.encoder.layer.7.output.dense.bias\n",
      "True backbone.encoder.layer.7.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.7.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.8.attention.self.query.weight\n",
      "True backbone.encoder.layer.8.attention.self.query.bias\n",
      "True backbone.encoder.layer.8.attention.self.key.weight\n",
      "True backbone.encoder.layer.8.attention.self.key.bias\n",
      "True backbone.encoder.layer.8.attention.self.value.weight\n",
      "True backbone.encoder.layer.8.attention.self.value.bias\n",
      "True backbone.encoder.layer.8.attention.output.dense.weight\n",
      "True backbone.encoder.layer.8.attention.output.dense.bias\n",
      "True backbone.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.8.intermediate.dense.weight\n",
      "True backbone.encoder.layer.8.intermediate.dense.bias\n",
      "True backbone.encoder.layer.8.output.dense.weight\n",
      "True backbone.encoder.layer.8.output.dense.bias\n",
      "True backbone.encoder.layer.8.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.8.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.9.attention.self.query.weight\n",
      "True backbone.encoder.layer.9.attention.self.query.bias\n",
      "True backbone.encoder.layer.9.attention.self.key.weight\n",
      "True backbone.encoder.layer.9.attention.self.key.bias\n",
      "True backbone.encoder.layer.9.attention.self.value.weight\n",
      "True backbone.encoder.layer.9.attention.self.value.bias\n",
      "True backbone.encoder.layer.9.attention.output.dense.weight\n",
      "True backbone.encoder.layer.9.attention.output.dense.bias\n",
      "True backbone.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.9.intermediate.dense.weight\n",
      "True backbone.encoder.layer.9.intermediate.dense.bias\n",
      "True backbone.encoder.layer.9.output.dense.weight\n",
      "True backbone.encoder.layer.9.output.dense.bias\n",
      "True backbone.encoder.layer.9.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.9.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.10.attention.self.query.weight\n",
      "True backbone.encoder.layer.10.attention.self.query.bias\n",
      "True backbone.encoder.layer.10.attention.self.key.weight\n",
      "True backbone.encoder.layer.10.attention.self.key.bias\n",
      "True backbone.encoder.layer.10.attention.self.value.weight\n",
      "True backbone.encoder.layer.10.attention.self.value.bias\n",
      "True backbone.encoder.layer.10.attention.output.dense.weight\n",
      "True backbone.encoder.layer.10.attention.output.dense.bias\n",
      "True backbone.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.10.intermediate.dense.weight\n",
      "True backbone.encoder.layer.10.intermediate.dense.bias\n",
      "True backbone.encoder.layer.10.output.dense.weight\n",
      "True backbone.encoder.layer.10.output.dense.bias\n",
      "True backbone.encoder.layer.10.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.10.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.11.attention.self.query.weight\n",
      "True backbone.encoder.layer.11.attention.self.query.bias\n",
      "True backbone.encoder.layer.11.attention.self.key.weight\n",
      "True backbone.encoder.layer.11.attention.self.key.bias\n",
      "True backbone.encoder.layer.11.attention.self.value.weight\n",
      "True backbone.encoder.layer.11.attention.self.value.bias\n",
      "True backbone.encoder.layer.11.attention.output.dense.weight\n",
      "True backbone.encoder.layer.11.attention.output.dense.bias\n",
      "True backbone.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.11.intermediate.dense.weight\n",
      "True backbone.encoder.layer.11.intermediate.dense.bias\n",
      "True backbone.encoder.layer.11.output.dense.weight\n",
      "True backbone.encoder.layer.11.output.dense.bias\n",
      "True backbone.encoder.layer.11.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.11.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.12.attention.self.query.weight\n",
      "True backbone.encoder.layer.12.attention.self.query.bias\n",
      "True backbone.encoder.layer.12.attention.self.key.weight\n",
      "True backbone.encoder.layer.12.attention.self.key.bias\n",
      "True backbone.encoder.layer.12.attention.self.value.weight\n",
      "True backbone.encoder.layer.12.attention.self.value.bias\n",
      "True backbone.encoder.layer.12.attention.output.dense.weight\n",
      "True backbone.encoder.layer.12.attention.output.dense.bias\n",
      "True backbone.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.12.intermediate.dense.weight\n",
      "True backbone.encoder.layer.12.intermediate.dense.bias\n",
      "True backbone.encoder.layer.12.output.dense.weight\n",
      "True backbone.encoder.layer.12.output.dense.bias\n",
      "True backbone.encoder.layer.12.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.12.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.13.attention.self.query.weight\n",
      "True backbone.encoder.layer.13.attention.self.query.bias\n",
      "True backbone.encoder.layer.13.attention.self.key.weight\n",
      "True backbone.encoder.layer.13.attention.self.key.bias\n",
      "True backbone.encoder.layer.13.attention.self.value.weight\n",
      "True backbone.encoder.layer.13.attention.self.value.bias\n",
      "True backbone.encoder.layer.13.attention.output.dense.weight\n",
      "True backbone.encoder.layer.13.attention.output.dense.bias\n",
      "True backbone.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.13.intermediate.dense.weight\n",
      "True backbone.encoder.layer.13.intermediate.dense.bias\n",
      "True backbone.encoder.layer.13.output.dense.weight\n",
      "True backbone.encoder.layer.13.output.dense.bias\n",
      "True backbone.encoder.layer.13.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.13.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.14.attention.self.query.weight\n",
      "True backbone.encoder.layer.14.attention.self.query.bias\n",
      "True backbone.encoder.layer.14.attention.self.key.weight\n",
      "True backbone.encoder.layer.14.attention.self.key.bias\n",
      "True backbone.encoder.layer.14.attention.self.value.weight\n",
      "True backbone.encoder.layer.14.attention.self.value.bias\n",
      "True backbone.encoder.layer.14.attention.output.dense.weight\n",
      "True backbone.encoder.layer.14.attention.output.dense.bias\n",
      "True backbone.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.14.intermediate.dense.weight\n",
      "True backbone.encoder.layer.14.intermediate.dense.bias\n",
      "True backbone.encoder.layer.14.output.dense.weight\n",
      "True backbone.encoder.layer.14.output.dense.bias\n",
      "True backbone.encoder.layer.14.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.14.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.15.attention.self.query.weight\n",
      "True backbone.encoder.layer.15.attention.self.query.bias\n",
      "True backbone.encoder.layer.15.attention.self.key.weight\n",
      "True backbone.encoder.layer.15.attention.self.key.bias\n",
      "True backbone.encoder.layer.15.attention.self.value.weight\n",
      "True backbone.encoder.layer.15.attention.self.value.bias\n",
      "True backbone.encoder.layer.15.attention.output.dense.weight\n",
      "True backbone.encoder.layer.15.attention.output.dense.bias\n",
      "True backbone.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.15.intermediate.dense.weight\n",
      "True backbone.encoder.layer.15.intermediate.dense.bias\n",
      "True backbone.encoder.layer.15.output.dense.weight\n",
      "True backbone.encoder.layer.15.output.dense.bias\n",
      "True backbone.encoder.layer.15.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.15.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.16.attention.self.query.weight\n",
      "True backbone.encoder.layer.16.attention.self.query.bias\n",
      "True backbone.encoder.layer.16.attention.self.key.weight\n",
      "True backbone.encoder.layer.16.attention.self.key.bias\n",
      "True backbone.encoder.layer.16.attention.self.value.weight\n",
      "True backbone.encoder.layer.16.attention.self.value.bias\n",
      "True backbone.encoder.layer.16.attention.output.dense.weight\n",
      "True backbone.encoder.layer.16.attention.output.dense.bias\n",
      "True backbone.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.16.intermediate.dense.weight\n",
      "True backbone.encoder.layer.16.intermediate.dense.bias\n",
      "True backbone.encoder.layer.16.output.dense.weight\n",
      "True backbone.encoder.layer.16.output.dense.bias\n",
      "True backbone.encoder.layer.16.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.16.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.17.attention.self.query.weight\n",
      "True backbone.encoder.layer.17.attention.self.query.bias\n",
      "True backbone.encoder.layer.17.attention.self.key.weight\n",
      "True backbone.encoder.layer.17.attention.self.key.bias\n",
      "True backbone.encoder.layer.17.attention.self.value.weight\n",
      "True backbone.encoder.layer.17.attention.self.value.bias\n",
      "True backbone.encoder.layer.17.attention.output.dense.weight\n",
      "True backbone.encoder.layer.17.attention.output.dense.bias\n",
      "True backbone.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.17.intermediate.dense.weight\n",
      "True backbone.encoder.layer.17.intermediate.dense.bias\n",
      "True backbone.encoder.layer.17.output.dense.weight\n",
      "True backbone.encoder.layer.17.output.dense.bias\n",
      "True backbone.encoder.layer.17.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.17.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.18.attention.self.query.weight\n",
      "True backbone.encoder.layer.18.attention.self.query.bias\n",
      "True backbone.encoder.layer.18.attention.self.key.weight\n",
      "True backbone.encoder.layer.18.attention.self.key.bias\n",
      "True backbone.encoder.layer.18.attention.self.value.weight\n",
      "True backbone.encoder.layer.18.attention.self.value.bias\n",
      "True backbone.encoder.layer.18.attention.output.dense.weight\n",
      "True backbone.encoder.layer.18.attention.output.dense.bias\n",
      "True backbone.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.18.intermediate.dense.weight\n",
      "True backbone.encoder.layer.18.intermediate.dense.bias\n",
      "True backbone.encoder.layer.18.output.dense.weight\n",
      "True backbone.encoder.layer.18.output.dense.bias\n",
      "True backbone.encoder.layer.18.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.18.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.19.attention.self.query.weight\n",
      "True backbone.encoder.layer.19.attention.self.query.bias\n",
      "True backbone.encoder.layer.19.attention.self.key.weight\n",
      "True backbone.encoder.layer.19.attention.self.key.bias\n",
      "True backbone.encoder.layer.19.attention.self.value.weight\n",
      "True backbone.encoder.layer.19.attention.self.value.bias\n",
      "True backbone.encoder.layer.19.attention.output.dense.weight\n",
      "True backbone.encoder.layer.19.attention.output.dense.bias\n",
      "True backbone.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.19.intermediate.dense.weight\n",
      "True backbone.encoder.layer.19.intermediate.dense.bias\n",
      "True backbone.encoder.layer.19.output.dense.weight\n",
      "True backbone.encoder.layer.19.output.dense.bias\n",
      "True backbone.encoder.layer.19.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.19.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.20.attention.self.query.weight\n",
      "True backbone.encoder.layer.20.attention.self.query.bias\n",
      "True backbone.encoder.layer.20.attention.self.key.weight\n",
      "True backbone.encoder.layer.20.attention.self.key.bias\n",
      "True backbone.encoder.layer.20.attention.self.value.weight\n",
      "True backbone.encoder.layer.20.attention.self.value.bias\n",
      "True backbone.encoder.layer.20.attention.output.dense.weight\n",
      "True backbone.encoder.layer.20.attention.output.dense.bias\n",
      "True backbone.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.20.intermediate.dense.weight\n",
      "True backbone.encoder.layer.20.intermediate.dense.bias\n",
      "True backbone.encoder.layer.20.output.dense.weight\n",
      "True backbone.encoder.layer.20.output.dense.bias\n",
      "True backbone.encoder.layer.20.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.20.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.21.attention.self.query.weight\n",
      "True backbone.encoder.layer.21.attention.self.query.bias\n",
      "True backbone.encoder.layer.21.attention.self.key.weight\n",
      "True backbone.encoder.layer.21.attention.self.key.bias\n",
      "True backbone.encoder.layer.21.attention.self.value.weight\n",
      "True backbone.encoder.layer.21.attention.self.value.bias\n",
      "True backbone.encoder.layer.21.attention.output.dense.weight\n",
      "True backbone.encoder.layer.21.attention.output.dense.bias\n",
      "True backbone.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.21.intermediate.dense.weight\n",
      "True backbone.encoder.layer.21.intermediate.dense.bias\n",
      "True backbone.encoder.layer.21.output.dense.weight\n",
      "True backbone.encoder.layer.21.output.dense.bias\n",
      "True backbone.encoder.layer.21.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.21.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.22.attention.self.query.weight\n",
      "True backbone.encoder.layer.22.attention.self.query.bias\n",
      "True backbone.encoder.layer.22.attention.self.key.weight\n",
      "True backbone.encoder.layer.22.attention.self.key.bias\n",
      "True backbone.encoder.layer.22.attention.self.value.weight\n",
      "True backbone.encoder.layer.22.attention.self.value.bias\n",
      "True backbone.encoder.layer.22.attention.output.dense.weight\n",
      "True backbone.encoder.layer.22.attention.output.dense.bias\n",
      "True backbone.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.22.intermediate.dense.weight\n",
      "True backbone.encoder.layer.22.intermediate.dense.bias\n",
      "True backbone.encoder.layer.22.output.dense.weight\n",
      "True backbone.encoder.layer.22.output.dense.bias\n",
      "True backbone.encoder.layer.22.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.22.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.23.attention.self.query.weight\n",
      "True backbone.encoder.layer.23.attention.self.query.bias\n",
      "True backbone.encoder.layer.23.attention.self.key.weight\n",
      "True backbone.encoder.layer.23.attention.self.key.bias\n",
      "True backbone.encoder.layer.23.attention.self.value.weight\n",
      "True backbone.encoder.layer.23.attention.self.value.bias\n",
      "True backbone.encoder.layer.23.attention.output.dense.weight\n",
      "True backbone.encoder.layer.23.attention.output.dense.bias\n",
      "True backbone.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "True backbone.encoder.layer.23.intermediate.dense.weight\n",
      "True backbone.encoder.layer.23.intermediate.dense.bias\n",
      "True backbone.encoder.layer.23.output.dense.weight\n",
      "True backbone.encoder.layer.23.output.dense.bias\n",
      "True backbone.encoder.layer.23.output.LayerNorm.weight\n",
      "True backbone.encoder.layer.23.output.LayerNorm.bias\n",
      "True backbone.pooler.dense.weight\n",
      "True backbone.pooler.dense.bias\n",
      "True linear.weight\n",
      "True linear.bias\n"
     ]
    }
   ],
   "source": [
    "for nam, mod in model.named_parameters():\n",
    "    print(mod.requires_grad, nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e79d943c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=4, bias=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.requires_grad_(False);\n",
    "model.linear.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c505ad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False backbone.embeddings.word_embeddings.weight\n",
      "False backbone.embeddings.position_embeddings.weight\n",
      "False backbone.embeddings.token_type_embeddings.weight\n",
      "False backbone.embeddings.LayerNorm.weight\n",
      "False backbone.embeddings.LayerNorm.bias\n",
      "False backbone.encoder.layer.0.attention.self.query.weight\n",
      "False backbone.encoder.layer.0.attention.self.query.bias\n",
      "False backbone.encoder.layer.0.attention.self.key.weight\n",
      "False backbone.encoder.layer.0.attention.self.key.bias\n",
      "False backbone.encoder.layer.0.attention.self.value.weight\n",
      "False backbone.encoder.layer.0.attention.self.value.bias\n",
      "False backbone.encoder.layer.0.attention.output.dense.weight\n",
      "False backbone.encoder.layer.0.attention.output.dense.bias\n",
      "False backbone.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.0.intermediate.dense.weight\n",
      "False backbone.encoder.layer.0.intermediate.dense.bias\n",
      "False backbone.encoder.layer.0.output.dense.weight\n",
      "False backbone.encoder.layer.0.output.dense.bias\n",
      "False backbone.encoder.layer.0.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.0.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.1.attention.self.query.weight\n",
      "False backbone.encoder.layer.1.attention.self.query.bias\n",
      "False backbone.encoder.layer.1.attention.self.key.weight\n",
      "False backbone.encoder.layer.1.attention.self.key.bias\n",
      "False backbone.encoder.layer.1.attention.self.value.weight\n",
      "False backbone.encoder.layer.1.attention.self.value.bias\n",
      "False backbone.encoder.layer.1.attention.output.dense.weight\n",
      "False backbone.encoder.layer.1.attention.output.dense.bias\n",
      "False backbone.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.1.intermediate.dense.weight\n",
      "False backbone.encoder.layer.1.intermediate.dense.bias\n",
      "False backbone.encoder.layer.1.output.dense.weight\n",
      "False backbone.encoder.layer.1.output.dense.bias\n",
      "False backbone.encoder.layer.1.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.1.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.2.attention.self.query.weight\n",
      "False backbone.encoder.layer.2.attention.self.query.bias\n",
      "False backbone.encoder.layer.2.attention.self.key.weight\n",
      "False backbone.encoder.layer.2.attention.self.key.bias\n",
      "False backbone.encoder.layer.2.attention.self.value.weight\n",
      "False backbone.encoder.layer.2.attention.self.value.bias\n",
      "False backbone.encoder.layer.2.attention.output.dense.weight\n",
      "False backbone.encoder.layer.2.attention.output.dense.bias\n",
      "False backbone.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.2.intermediate.dense.weight\n",
      "False backbone.encoder.layer.2.intermediate.dense.bias\n",
      "False backbone.encoder.layer.2.output.dense.weight\n",
      "False backbone.encoder.layer.2.output.dense.bias\n",
      "False backbone.encoder.layer.2.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.2.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.3.attention.self.query.weight\n",
      "False backbone.encoder.layer.3.attention.self.query.bias\n",
      "False backbone.encoder.layer.3.attention.self.key.weight\n",
      "False backbone.encoder.layer.3.attention.self.key.bias\n",
      "False backbone.encoder.layer.3.attention.self.value.weight\n",
      "False backbone.encoder.layer.3.attention.self.value.bias\n",
      "False backbone.encoder.layer.3.attention.output.dense.weight\n",
      "False backbone.encoder.layer.3.attention.output.dense.bias\n",
      "False backbone.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.3.intermediate.dense.weight\n",
      "False backbone.encoder.layer.3.intermediate.dense.bias\n",
      "False backbone.encoder.layer.3.output.dense.weight\n",
      "False backbone.encoder.layer.3.output.dense.bias\n",
      "False backbone.encoder.layer.3.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.3.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.4.attention.self.query.weight\n",
      "False backbone.encoder.layer.4.attention.self.query.bias\n",
      "False backbone.encoder.layer.4.attention.self.key.weight\n",
      "False backbone.encoder.layer.4.attention.self.key.bias\n",
      "False backbone.encoder.layer.4.attention.self.value.weight\n",
      "False backbone.encoder.layer.4.attention.self.value.bias\n",
      "False backbone.encoder.layer.4.attention.output.dense.weight\n",
      "False backbone.encoder.layer.4.attention.output.dense.bias\n",
      "False backbone.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.4.intermediate.dense.weight\n",
      "False backbone.encoder.layer.4.intermediate.dense.bias\n",
      "False backbone.encoder.layer.4.output.dense.weight\n",
      "False backbone.encoder.layer.4.output.dense.bias\n",
      "False backbone.encoder.layer.4.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.4.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.5.attention.self.query.weight\n",
      "False backbone.encoder.layer.5.attention.self.query.bias\n",
      "False backbone.encoder.layer.5.attention.self.key.weight\n",
      "False backbone.encoder.layer.5.attention.self.key.bias\n",
      "False backbone.encoder.layer.5.attention.self.value.weight\n",
      "False backbone.encoder.layer.5.attention.self.value.bias\n",
      "False backbone.encoder.layer.5.attention.output.dense.weight\n",
      "False backbone.encoder.layer.5.attention.output.dense.bias\n",
      "False backbone.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.5.intermediate.dense.weight\n",
      "False backbone.encoder.layer.5.intermediate.dense.bias\n",
      "False backbone.encoder.layer.5.output.dense.weight\n",
      "False backbone.encoder.layer.5.output.dense.bias\n",
      "False backbone.encoder.layer.5.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.5.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.6.attention.self.query.weight\n",
      "False backbone.encoder.layer.6.attention.self.query.bias\n",
      "False backbone.encoder.layer.6.attention.self.key.weight\n",
      "False backbone.encoder.layer.6.attention.self.key.bias\n",
      "False backbone.encoder.layer.6.attention.self.value.weight\n",
      "False backbone.encoder.layer.6.attention.self.value.bias\n",
      "False backbone.encoder.layer.6.attention.output.dense.weight\n",
      "False backbone.encoder.layer.6.attention.output.dense.bias\n",
      "False backbone.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.6.intermediate.dense.weight\n",
      "False backbone.encoder.layer.6.intermediate.dense.bias\n",
      "False backbone.encoder.layer.6.output.dense.weight\n",
      "False backbone.encoder.layer.6.output.dense.bias\n",
      "False backbone.encoder.layer.6.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.6.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.7.attention.self.query.weight\n",
      "False backbone.encoder.layer.7.attention.self.query.bias\n",
      "False backbone.encoder.layer.7.attention.self.key.weight\n",
      "False backbone.encoder.layer.7.attention.self.key.bias\n",
      "False backbone.encoder.layer.7.attention.self.value.weight\n",
      "False backbone.encoder.layer.7.attention.self.value.bias\n",
      "False backbone.encoder.layer.7.attention.output.dense.weight\n",
      "False backbone.encoder.layer.7.attention.output.dense.bias\n",
      "False backbone.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.7.intermediate.dense.weight\n",
      "False backbone.encoder.layer.7.intermediate.dense.bias\n",
      "False backbone.encoder.layer.7.output.dense.weight\n",
      "False backbone.encoder.layer.7.output.dense.bias\n",
      "False backbone.encoder.layer.7.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.7.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.8.attention.self.query.weight\n",
      "False backbone.encoder.layer.8.attention.self.query.bias\n",
      "False backbone.encoder.layer.8.attention.self.key.weight\n",
      "False backbone.encoder.layer.8.attention.self.key.bias\n",
      "False backbone.encoder.layer.8.attention.self.value.weight\n",
      "False backbone.encoder.layer.8.attention.self.value.bias\n",
      "False backbone.encoder.layer.8.attention.output.dense.weight\n",
      "False backbone.encoder.layer.8.attention.output.dense.bias\n",
      "False backbone.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.8.intermediate.dense.weight\n",
      "False backbone.encoder.layer.8.intermediate.dense.bias\n",
      "False backbone.encoder.layer.8.output.dense.weight\n",
      "False backbone.encoder.layer.8.output.dense.bias\n",
      "False backbone.encoder.layer.8.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.8.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.9.attention.self.query.weight\n",
      "False backbone.encoder.layer.9.attention.self.query.bias\n",
      "False backbone.encoder.layer.9.attention.self.key.weight\n",
      "False backbone.encoder.layer.9.attention.self.key.bias\n",
      "False backbone.encoder.layer.9.attention.self.value.weight\n",
      "False backbone.encoder.layer.9.attention.self.value.bias\n",
      "False backbone.encoder.layer.9.attention.output.dense.weight\n",
      "False backbone.encoder.layer.9.attention.output.dense.bias\n",
      "False backbone.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.9.intermediate.dense.weight\n",
      "False backbone.encoder.layer.9.intermediate.dense.bias\n",
      "False backbone.encoder.layer.9.output.dense.weight\n",
      "False backbone.encoder.layer.9.output.dense.bias\n",
      "False backbone.encoder.layer.9.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.9.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.10.attention.self.query.weight\n",
      "False backbone.encoder.layer.10.attention.self.query.bias\n",
      "False backbone.encoder.layer.10.attention.self.key.weight\n",
      "False backbone.encoder.layer.10.attention.self.key.bias\n",
      "False backbone.encoder.layer.10.attention.self.value.weight\n",
      "False backbone.encoder.layer.10.attention.self.value.bias\n",
      "False backbone.encoder.layer.10.attention.output.dense.weight\n",
      "False backbone.encoder.layer.10.attention.output.dense.bias\n",
      "False backbone.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.10.intermediate.dense.weight\n",
      "False backbone.encoder.layer.10.intermediate.dense.bias\n",
      "False backbone.encoder.layer.10.output.dense.weight\n",
      "False backbone.encoder.layer.10.output.dense.bias\n",
      "False backbone.encoder.layer.10.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.10.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.11.attention.self.query.weight\n",
      "False backbone.encoder.layer.11.attention.self.query.bias\n",
      "False backbone.encoder.layer.11.attention.self.key.weight\n",
      "False backbone.encoder.layer.11.attention.self.key.bias\n",
      "False backbone.encoder.layer.11.attention.self.value.weight\n",
      "False backbone.encoder.layer.11.attention.self.value.bias\n",
      "False backbone.encoder.layer.11.attention.output.dense.weight\n",
      "False backbone.encoder.layer.11.attention.output.dense.bias\n",
      "False backbone.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.11.intermediate.dense.weight\n",
      "False backbone.encoder.layer.11.intermediate.dense.bias\n",
      "False backbone.encoder.layer.11.output.dense.weight\n",
      "False backbone.encoder.layer.11.output.dense.bias\n",
      "False backbone.encoder.layer.11.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.11.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.12.attention.self.query.weight\n",
      "False backbone.encoder.layer.12.attention.self.query.bias\n",
      "False backbone.encoder.layer.12.attention.self.key.weight\n",
      "False backbone.encoder.layer.12.attention.self.key.bias\n",
      "False backbone.encoder.layer.12.attention.self.value.weight\n",
      "False backbone.encoder.layer.12.attention.self.value.bias\n",
      "False backbone.encoder.layer.12.attention.output.dense.weight\n",
      "False backbone.encoder.layer.12.attention.output.dense.bias\n",
      "False backbone.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.12.intermediate.dense.weight\n",
      "False backbone.encoder.layer.12.intermediate.dense.bias\n",
      "False backbone.encoder.layer.12.output.dense.weight\n",
      "False backbone.encoder.layer.12.output.dense.bias\n",
      "False backbone.encoder.layer.12.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.12.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.13.attention.self.query.weight\n",
      "False backbone.encoder.layer.13.attention.self.query.bias\n",
      "False backbone.encoder.layer.13.attention.self.key.weight\n",
      "False backbone.encoder.layer.13.attention.self.key.bias\n",
      "False backbone.encoder.layer.13.attention.self.value.weight\n",
      "False backbone.encoder.layer.13.attention.self.value.bias\n",
      "False backbone.encoder.layer.13.attention.output.dense.weight\n",
      "False backbone.encoder.layer.13.attention.output.dense.bias\n",
      "False backbone.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.13.intermediate.dense.weight\n",
      "False backbone.encoder.layer.13.intermediate.dense.bias\n",
      "False backbone.encoder.layer.13.output.dense.weight\n",
      "False backbone.encoder.layer.13.output.dense.bias\n",
      "False backbone.encoder.layer.13.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.13.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.14.attention.self.query.weight\n",
      "False backbone.encoder.layer.14.attention.self.query.bias\n",
      "False backbone.encoder.layer.14.attention.self.key.weight\n",
      "False backbone.encoder.layer.14.attention.self.key.bias\n",
      "False backbone.encoder.layer.14.attention.self.value.weight\n",
      "False backbone.encoder.layer.14.attention.self.value.bias\n",
      "False backbone.encoder.layer.14.attention.output.dense.weight\n",
      "False backbone.encoder.layer.14.attention.output.dense.bias\n",
      "False backbone.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.14.intermediate.dense.weight\n",
      "False backbone.encoder.layer.14.intermediate.dense.bias\n",
      "False backbone.encoder.layer.14.output.dense.weight\n",
      "False backbone.encoder.layer.14.output.dense.bias\n",
      "False backbone.encoder.layer.14.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.14.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.15.attention.self.query.weight\n",
      "False backbone.encoder.layer.15.attention.self.query.bias\n",
      "False backbone.encoder.layer.15.attention.self.key.weight\n",
      "False backbone.encoder.layer.15.attention.self.key.bias\n",
      "False backbone.encoder.layer.15.attention.self.value.weight\n",
      "False backbone.encoder.layer.15.attention.self.value.bias\n",
      "False backbone.encoder.layer.15.attention.output.dense.weight\n",
      "False backbone.encoder.layer.15.attention.output.dense.bias\n",
      "False backbone.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.15.intermediate.dense.weight\n",
      "False backbone.encoder.layer.15.intermediate.dense.bias\n",
      "False backbone.encoder.layer.15.output.dense.weight\n",
      "False backbone.encoder.layer.15.output.dense.bias\n",
      "False backbone.encoder.layer.15.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.15.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.16.attention.self.query.weight\n",
      "False backbone.encoder.layer.16.attention.self.query.bias\n",
      "False backbone.encoder.layer.16.attention.self.key.weight\n",
      "False backbone.encoder.layer.16.attention.self.key.bias\n",
      "False backbone.encoder.layer.16.attention.self.value.weight\n",
      "False backbone.encoder.layer.16.attention.self.value.bias\n",
      "False backbone.encoder.layer.16.attention.output.dense.weight\n",
      "False backbone.encoder.layer.16.attention.output.dense.bias\n",
      "False backbone.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.16.intermediate.dense.weight\n",
      "False backbone.encoder.layer.16.intermediate.dense.bias\n",
      "False backbone.encoder.layer.16.output.dense.weight\n",
      "False backbone.encoder.layer.16.output.dense.bias\n",
      "False backbone.encoder.layer.16.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.16.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.17.attention.self.query.weight\n",
      "False backbone.encoder.layer.17.attention.self.query.bias\n",
      "False backbone.encoder.layer.17.attention.self.key.weight\n",
      "False backbone.encoder.layer.17.attention.self.key.bias\n",
      "False backbone.encoder.layer.17.attention.self.value.weight\n",
      "False backbone.encoder.layer.17.attention.self.value.bias\n",
      "False backbone.encoder.layer.17.attention.output.dense.weight\n",
      "False backbone.encoder.layer.17.attention.output.dense.bias\n",
      "False backbone.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.17.intermediate.dense.weight\n",
      "False backbone.encoder.layer.17.intermediate.dense.bias\n",
      "False backbone.encoder.layer.17.output.dense.weight\n",
      "False backbone.encoder.layer.17.output.dense.bias\n",
      "False backbone.encoder.layer.17.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.17.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.18.attention.self.query.weight\n",
      "False backbone.encoder.layer.18.attention.self.query.bias\n",
      "False backbone.encoder.layer.18.attention.self.key.weight\n",
      "False backbone.encoder.layer.18.attention.self.key.bias\n",
      "False backbone.encoder.layer.18.attention.self.value.weight\n",
      "False backbone.encoder.layer.18.attention.self.value.bias\n",
      "False backbone.encoder.layer.18.attention.output.dense.weight\n",
      "False backbone.encoder.layer.18.attention.output.dense.bias\n",
      "False backbone.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.18.intermediate.dense.weight\n",
      "False backbone.encoder.layer.18.intermediate.dense.bias\n",
      "False backbone.encoder.layer.18.output.dense.weight\n",
      "False backbone.encoder.layer.18.output.dense.bias\n",
      "False backbone.encoder.layer.18.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.18.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.19.attention.self.query.weight\n",
      "False backbone.encoder.layer.19.attention.self.query.bias\n",
      "False backbone.encoder.layer.19.attention.self.key.weight\n",
      "False backbone.encoder.layer.19.attention.self.key.bias\n",
      "False backbone.encoder.layer.19.attention.self.value.weight\n",
      "False backbone.encoder.layer.19.attention.self.value.bias\n",
      "False backbone.encoder.layer.19.attention.output.dense.weight\n",
      "False backbone.encoder.layer.19.attention.output.dense.bias\n",
      "False backbone.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.19.intermediate.dense.weight\n",
      "False backbone.encoder.layer.19.intermediate.dense.bias\n",
      "False backbone.encoder.layer.19.output.dense.weight\n",
      "False backbone.encoder.layer.19.output.dense.bias\n",
      "False backbone.encoder.layer.19.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.19.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.20.attention.self.query.weight\n",
      "False backbone.encoder.layer.20.attention.self.query.bias\n",
      "False backbone.encoder.layer.20.attention.self.key.weight\n",
      "False backbone.encoder.layer.20.attention.self.key.bias\n",
      "False backbone.encoder.layer.20.attention.self.value.weight\n",
      "False backbone.encoder.layer.20.attention.self.value.bias\n",
      "False backbone.encoder.layer.20.attention.output.dense.weight\n",
      "False backbone.encoder.layer.20.attention.output.dense.bias\n",
      "False backbone.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.20.intermediate.dense.weight\n",
      "False backbone.encoder.layer.20.intermediate.dense.bias\n",
      "False backbone.encoder.layer.20.output.dense.weight\n",
      "False backbone.encoder.layer.20.output.dense.bias\n",
      "False backbone.encoder.layer.20.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.20.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.21.attention.self.query.weight\n",
      "False backbone.encoder.layer.21.attention.self.query.bias\n",
      "False backbone.encoder.layer.21.attention.self.key.weight\n",
      "False backbone.encoder.layer.21.attention.self.key.bias\n",
      "False backbone.encoder.layer.21.attention.self.value.weight\n",
      "False backbone.encoder.layer.21.attention.self.value.bias\n",
      "False backbone.encoder.layer.21.attention.output.dense.weight\n",
      "False backbone.encoder.layer.21.attention.output.dense.bias\n",
      "False backbone.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.21.intermediate.dense.weight\n",
      "False backbone.encoder.layer.21.intermediate.dense.bias\n",
      "False backbone.encoder.layer.21.output.dense.weight\n",
      "False backbone.encoder.layer.21.output.dense.bias\n",
      "False backbone.encoder.layer.21.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.21.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.22.attention.self.query.weight\n",
      "False backbone.encoder.layer.22.attention.self.query.bias\n",
      "False backbone.encoder.layer.22.attention.self.key.weight\n",
      "False backbone.encoder.layer.22.attention.self.key.bias\n",
      "False backbone.encoder.layer.22.attention.self.value.weight\n",
      "False backbone.encoder.layer.22.attention.self.value.bias\n",
      "False backbone.encoder.layer.22.attention.output.dense.weight\n",
      "False backbone.encoder.layer.22.attention.output.dense.bias\n",
      "False backbone.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.22.intermediate.dense.weight\n",
      "False backbone.encoder.layer.22.intermediate.dense.bias\n",
      "False backbone.encoder.layer.22.output.dense.weight\n",
      "False backbone.encoder.layer.22.output.dense.bias\n",
      "False backbone.encoder.layer.22.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.22.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.23.attention.self.query.weight\n",
      "False backbone.encoder.layer.23.attention.self.query.bias\n",
      "False backbone.encoder.layer.23.attention.self.key.weight\n",
      "False backbone.encoder.layer.23.attention.self.key.bias\n",
      "False backbone.encoder.layer.23.attention.self.value.weight\n",
      "False backbone.encoder.layer.23.attention.self.value.bias\n",
      "False backbone.encoder.layer.23.attention.output.dense.weight\n",
      "False backbone.encoder.layer.23.attention.output.dense.bias\n",
      "False backbone.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "False backbone.encoder.layer.23.intermediate.dense.weight\n",
      "False backbone.encoder.layer.23.intermediate.dense.bias\n",
      "False backbone.encoder.layer.23.output.dense.weight\n",
      "False backbone.encoder.layer.23.output.dense.bias\n",
      "False backbone.encoder.layer.23.output.LayerNorm.weight\n",
      "False backbone.encoder.layer.23.output.LayerNorm.bias\n",
      "False backbone.pooler.dense.weight\n",
      "False backbone.pooler.dense.bias\n",
      "True linear.weight\n",
      "True linear.bias\n"
     ]
    }
   ],
   "source": [
    "for nam, mod in model.named_parameters():\n",
    "    print(mod.requires_grad, nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59bba162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqClassifier(\n",
       "  (backbone): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9913b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = torch.utils.data.DataLoader(DatasetText(df_train), batch_size=2, shuffle=True, \n",
    "#                                                    pin_memory=False,\n",
    "#                                                    num_workers=0,\n",
    "#                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e12d81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (next(iter(train_dataloader))[0]['input_ids']).squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbc9e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(   \n",
    "#     (next(iter(train_dataloader))[0]['input_ids']).squeeze(1), \n",
    "#       (next(iter(train_dataloader))[0]['attention_mask']).squeeze(1)  \n",
    "#      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76778cfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labels: 100%|███████████████████████████████████████████████████████████████| 10317/10317 [00:00<00:00, 5159489.01it/s]\n",
      "Tokens: 100%|██████████████████████████████████████████████████████████████████| 10317/10317 [00:02<00:00, 3955.91it/s]\n",
      "Labels: 100%|██████████████████████████████████████████████████████████████████████████████| 1290/1290 [00:00<?, ?it/s]\n",
      "Tokens: 100%|████████████████████████████████████████████████████████████████████| 1290/1290 [00:00<00:00, 4147.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5159/5159 [11:38<00:00,  7.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [01:22<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model, epoch-0\n",
      "Epochs: 1 | Train Loss:  0.471                 | Train Accuracy:  0.628                 | Val Loss:  0.372                 | Val Accuracy:  0.769                 | Val ROCAUC:  0.851 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5159/5159 [11:37<00:00,  7.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [01:22<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model, epoch-1\n",
      "Epochs: 2 | Train Loss:  0.436                 | Train Accuracy:  0.674                 | Val Loss:  0.361                 | Val Accuracy:  0.760                 | Val ROCAUC:  0.862 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5159/5159 [11:37<00:00,  7.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [01:22<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model, epoch-2\n",
      "Epochs: 3 | Train Loss:  0.433                 | Train Accuracy:  0.671                 | Val Loss:  0.347                 | Val Accuracy:  0.759                 | Val ROCAUC:  0.868 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5159/5159 [11:36<00:00,  7.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [01:22<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model, epoch-3\n",
      "Epochs: 4 | Train Loss:  0.439                 | Train Accuracy:  0.659                 | Val Loss:  0.343                 | Val Accuracy:  0.767                 | Val ROCAUC:  0.871 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5159/5159 [11:49<00:00,  7.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [01:22<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.434                 | Train Accuracy:  0.663                 | Val Loss:  0.339                 | Val Accuracy:  0.779                 | Val ROCAUC:  0.870 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5159/5159 [11:37<00:00,  7.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [01:22<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.430                 | Train Accuracy:  0.674                 | Val Loss:  0.367                 | Val Accuracy:  0.733                 | Val ROCAUC:  0.867 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5159/5159 [11:36<00:00,  7.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [01:22<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model, epoch-6\n",
      "Epochs: 7 | Train Loss:  0.434                 | Train Accuracy:  0.672                 | Val Loss:  0.356                 | Val Accuracy:  0.728                 | Val ROCAUC:  0.871 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5159/5159 [11:37<00:00,  7.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [01:22<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model, epoch-7\n",
      "Epochs: 8 | Train Loss:  0.437                 | Train Accuracy:  0.667                 | Val Loss:  0.339                 | Val Accuracy:  0.777                 | Val ROCAUC:  0.873 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5159/5159 [11:41<00:00,  7.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [01:22<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model, epoch-8\n",
      "Epochs: 9 | Train Loss:  0.437                 | Train Accuracy:  0.668                 | Val Loss:  0.344                 | Val Accuracy:  0.752                 | Val ROCAUC:  0.875 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5159/5159 [11:38<00:00,  7.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [01:22<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.438                 | Train Accuracy:  0.666                 | Val Loss:  0.341                 | Val Accuracy:  0.776                 | Val ROCAUC:  0.875 \n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "BS = 2\n",
    "\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.mkdir('checkpoints')\n",
    "\n",
    "# start_time_str = get_datetime_str()\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    \n",
    "    best_model = copy.deepcopy(model.cpu())\n",
    "    \n",
    "    train, val = DatasetText(train_data), DatasetText(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=BS, \n",
    "                                                   shuffle=True, \n",
    "                                                   pin_memory=True,\n",
    "                                                   num_workers=0,\n",
    "                                                  )\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=BS,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=0,\n",
    "                                                )\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    best_val = 0\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        \n",
    "            model_outputs = {'train':[], 'val':[]} \n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            \n",
    "            model.train()\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "#                 model_outputs['train'].append(output.detach().cpu().numpy())\n",
    "                \n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in tqdm(val_dataloader):\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                    \n",
    "                    \n",
    "                    output = model(input_id, mask)\n",
    "                    model_outputs['val'].append(output.cpu().numpy())\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "                    \n",
    "                metric_val = METRIC(list(map(lambda x: labels[x], val_data[TARGET])), model_outputs['val'])\n",
    "            \n",
    "            if best_val < metric_val:\n",
    "                best_val = metric_val\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print(f\"Saving the best model, epoch-{epoch_num}\")\n",
    "                torch.save(best_model, f'checkpoints/best_seq_model' )                \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f} \\\n",
    "                | Val ROCAUC: {metric_val: .3f} ' )\n",
    "            \n",
    "    return best_model\n",
    "                  \n",
    "EPOCHS = 10\n",
    "LR = 5e-4\n",
    "\n",
    "\n",
    "# Start fine-tuning from zero\n",
    "# model = SeqClassifier(num_out_classes=len(labels))\n",
    "\n",
    "my_best_model = train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_best_model = torch.load(\"checkpoints/best_seq_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue fine-tuning\n",
    "# my_best_model = train(my_best_model, df_train, df_val, LR, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    \n",
    "    y_true = list(map(lambda x: labels[x], test_data.sentiment.values))\n",
    "\n",
    "    test = DatasetText(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model_outputs = []\n",
    "\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "            \n",
    "              model_outputs.append(output.cpu().numpy())\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "                \n",
    "        model_outputs = np.array(model_outputs)\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    print(f'Test RocAuc: {METRIC(y_true, model_outputs): .3f}')\n",
    "    \n",
    "    \n",
    "    \n",
    "# evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_unfrozen_bert = torch.load(\"checkpoints/best_seq_model-unfrozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa637d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(my_unfrozen_bert, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da036de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing nan class column\n",
    "preds_classes = np.delete(y_pred.reshape(-1,4).squeeze(), 0, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38f699f9",
   "metadata": {},
   "source": [
    "sberbank-ai/ruRoberta-large\n",
    "#  Frozen\n",
    "Test RocAuc: 0.893\n",
    "#  Unfrozen \n",
    "Test Accuracy:  0.840\n",
    "Test RocAuc:  0.912"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27688d",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath_test = 'test_texts.csv'\n",
    "df_test = pd.read_csv(datapath_test)\n",
    "df_test = df_test.sort_values(by='id')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_reverse = {val:key for key, val in labels.items()}\n",
    "labels_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916edb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetText_Test(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.texts = [tokenizer(text, \n",
    "                                padding='max_length', \n",
    "                                max_length = 512, \n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\"\n",
    "                               ) \n",
    "                      for text in tqdm(df['text'], desc='Tokens')\n",
    "                     ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "\n",
    "        return batch_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ddd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model, test_data):\n",
    "    out_pred = []\n",
    "\n",
    "    test = DatasetText_Test(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input in tqdm(test_dataloader, desc=\"Predicting test\"):\n",
    "\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask).detach().cpu().numpy()\n",
    "            pred_labels = np.argmax(output, axis=1)\n",
    "            authors = [labels_reverse[pred_] for pred_ in pred_labels]\n",
    "            out_pred.append(authors)\n",
    "            \n",
    "    return np.ravel(out_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d78c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = predict_test(my_best_model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db09ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy = df_test.copy()\n",
    "df_test_copy[TARGET] = preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d451ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save = df_test_copy[[\"id\", \"author\"]]\n",
    "df_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Submissions'):\n",
    "    os.mkdir('Submissions')\n",
    "    \n",
    "df_save.to_csv(\n",
    "    os.path.join(\"Submissions\", \"submission.csv\"), \n",
    "    index=False,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118570a9",
   "metadata": {},
   "source": [
    "#  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmpttv-jup",
   "language": "python",
   "name": "cmpttv-jup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
