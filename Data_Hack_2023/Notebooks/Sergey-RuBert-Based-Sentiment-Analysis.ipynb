{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "cellId": "7a8qio899ioc6pdmd61da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (0.25.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.19.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.9.1+cu111)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (0.22.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.4.5)\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.8/dist-packages (0.6.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.14.0)\n",
      "Requirement already satisfied: datasets in /home/jupyter/.local/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: seaborn in /home/jupyter/.local/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.8/dist-packages (1.4.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /kernel/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from nltk) (1.16.0)\n",
      "Requirement already satisfied: packaging in /kernel/lib/python3.8/site-packages (from torchmetrics) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: xxhash in /kernel/fallback/lib/python3.8/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.10.15)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2021.11.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.8/dist-packages (from seaborn) (3.1.3)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.18.2)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.3.1)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2.6.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /kernel/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /kernel/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /kernel/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /kernel/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /kernel/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (51.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.43.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.19.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (5.2.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /kernel/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /kernel/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (6.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /kernel/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
      "Installing collected packages: charset-normalizer\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "moto 1.3.14 requires idna<2.9,>=2.5, but you have idna 3.4 which is incompatible.\n",
      "kaggle 1.5.8 requires urllib3<1.25,>=1.21.1, but you have urllib3 1.26.15 which is incompatible.\n",
      "cloud-ml 0.0.1 requires requests<=2.25.1,>=2.22.0, but you have requests 2.28.2 which is incompatible.\n",
      "cloud-ml 0.0.1 requires tqdm<=4.54.1,>=4.45.0, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\n",
      "Successfully installed charset-normalizer-2.1.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install pandas numpy torch scikit-learn nltk torchmetrics transformers datasets seaborn pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "cellId": "kkd0uilrda8bsgu8fe8zk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "dataset_path = \"/home/jupyter/mnt/s3/databucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "cellId": "gxxkt3ywrfiegydwxg6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mUnigram Train-Copy1.png\u001b[0m*\n",
      "\u001b[01;32mUnigram Train.png\u001b[0m*\n",
      "\u001b[34;42mUntitled Folder\u001b[0m/\n",
      "\u001b[01;32mbank-categories.csv\u001b[0m*\n",
      "\u001b[01;32mbank-sentiment.csv\u001b[0m*\n",
      "\u001b[01;32mbanki.csv\u001b[0m*\n",
      "\u001b[01;32mbanks-ethic-DB - NEW.docx\u001b[0m*\n",
      "\u001b[34;42mcheckpoints\u001b[0m/\n",
      "\u001b[01;32mfoo.pdf\u001b[0m*\n",
      "\u001b[01;32mfoo.png\u001b[0m*\n",
      "\u001b[34;42mlightning_logs\u001b[0m/\n",
      "\u001b[01;32mpie.png\u001b[0m*\n",
      "\u001b[01;32msentiment.csv\u001b[0m*\n",
      "\u001b[34;42msource\u001b[0m/\n",
      "\u001b[01;32mto.png\u001b[0m*\n",
      "\u001b[01;32mtrain.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%ls $dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "jve8qtc61lgkn67aug0lu",
    "execution_id": "d6ee2fe9-9c8d-4752-ac6a-7c2055339f2a"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "cellId": "elyycn6w2q7mi322a5078m"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torchmetrics import AUROC\n",
    "from torchmetrics import ROC\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "cellId": "917za5dikm597x1hxrqrbh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from functools import lru_cache\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import copy\n",
    "import random\n",
    "import shutil\n",
    "import requests\n",
    "import subprocess\n",
    "from urllib.request import urlretrieve\n",
    "from typing import Any, Callable, Dict, Iterable, List, Tuple\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pytorch_lightning as pl\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "# Библиотеки для визуализации\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from joblib import load, dump\n",
    "\n",
    "# seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "t1vklfrybq95qn9bcnwl6",
    "execution_id": "6008ffb4-f826-4329-8717-4260ee294134"
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "cellId": "86pfhjtavad6besax15tv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/mnt/s3/databucket/banki.csv'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "data = os.path.join(dataset_path, 'banki.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "b5n0068lvub2lxmddal3ls",
    "execution_id": "fd84f632-73b3-40dd-9165-c43a6aed8172"
   },
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "cellId": "ar0i4fks2gelpo1xufkhbj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "tweets_col_number = 3\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(dataset_path, 'train.csv'), index_col=0)\n",
    "#positive_tweets = pd.read_csv(\n",
    "#    'positive.csv', header=None, delimiter=';')[[tweets_col_number]]\n",
    "dataset_text=dataset.drop(['1category','2category'],axis=1)\n",
    "data_positive=dataset_text[dataset_text['sentiment']=='+']\n",
    "data_negative=dataset_text[dataset_text['sentiment']=='−']\n",
    "data_neutral=dataset_text[dataset_text['sentiment']=='?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "cellId": "dxw91tffcoupjyyxubgv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6262 10192 2907\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(len(data_positive),len(data_negative),len(data_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "cellId": "m86pvxk9twd86ktg61hfbq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>2category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13007</th>\n",
       "      <td>Начну с того, что я пользовался и пользуюсь ус...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18828</th>\n",
       "      <td>Точка идеально походит для таких «чайников» ка...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19214</th>\n",
       "      <td>Открывали счет 2 недели... Открыли, пока готов...</td>\n",
       "      <td>Price</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19220</th>\n",
       "      <td>Итого что имеем обещанная ставка выросла более...</td>\n",
       "      <td>Price</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18944</th>\n",
       "      <td>Резюме: не ходите в Росбанк, он очень непорядо...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12378</th>\n",
       "      <td>Почему работают неквалифицированные специалист...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18821</th>\n",
       "      <td>Это реально круто.2) Очень грамотные менеджеры...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12593</th>\n",
       "      <td>Ответа Банка я так и не получила, и, хуже того...</td>\n",
       "      <td>Safety</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>* Удобство: 10 из 10* Работа сотрудников: 10 и...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18801</th>\n",
       "      <td>Первое знакомство с новым для меня банком прош...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  ... sentiment\n",
       "13007  Начну с того, что я пользовался и пользуюсь ус...  ...         +\n",
       "18828  Точка идеально походит для таких «чайников» ка...  ...         +\n",
       "19214  Открывали счет 2 недели... Открыли, пока готов...  ...         −\n",
       "19220  Итого что имеем обещанная ставка выросла более...  ...         −\n",
       "18944  Резюме: не ходите в Росбанк, он очень непорядо...  ...         −\n",
       "...                                                  ...  ...       ...\n",
       "12378  Почему работают неквалифицированные специалист...  ...         −\n",
       "18821  Это реально круто.2) Очень грамотные менеджеры...  ...         +\n",
       "12593  Ответа Банка я так и не получила, и, хуже того...  ...         −\n",
       "4600   * Удобство: 10 из 10* Работа сотрудников: 10 и...  ...         +\n",
       "18801  Первое знакомство с новым для меня банком прош...  ...         +\n",
       "\n",
       "[999 rows x 4 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "double_2cat = dataset[~dataset[\"2category\"].isna()]\n",
    "double_2cat[\"1category\"] = double_2cat[\"2category\"]\n",
    "double_2cat[\"2category\"] = np.nan\n",
    "double_2cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "cellId": "0kkn6jgfri4s1o9sd755f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>При этом всегда получал качественные услуги.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>Не вижу, за что хотя бы 2 поставить, сервис на 1!</td>\n",
       "      <td>?</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>Вот так \"Мой любимый\" банк МКБ меня обманул.</td>\n",
       "      <td>?</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11640</th>\n",
       "      <td>Отвратительное отношение к клиентам.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>Всегда в любое время дня и ночи помогут, ответ...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19199</th>\n",
       "      <td>В очереди, кстати, узнал, что многие также зак...</td>\n",
       "      <td>Price</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12512</th>\n",
       "      <td>Мало того, что услуги навязали, так еще и вопр...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19056</th>\n",
       "      <td>Почему-то с этим банком постоянно возникают пр...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12940</th>\n",
       "      <td>Что по статистике такие суммы сейчас вообще не...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18693</th>\n",
       "      <td>Но уже через 40 минут дзилинь и новая ссылка в...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12871 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  ... sentiment\n",
       "4754        При этом всегда получал качественные услуги.  ...         +\n",
       "4417   Не вижу, за что хотя бы 2 поставить, сервис на 1!  ...         −\n",
       "3629        Вот так \"Мой любимый\" банк МКБ меня обманул.  ...         −\n",
       "11640               Отвратительное отношение к клиентам.  ...         −\n",
       "5571   Всегда в любое время дня и ночи помогут, ответ...  ...         +\n",
       "...                                                  ...  ...       ...\n",
       "19199  В очереди, кстати, узнал, что многие также зак...  ...         −\n",
       "12512  Мало того, что услуги навязали, так еще и вопр...  ...         −\n",
       "19056  Почему-то с этим банком постоянно возникают пр...  ...         −\n",
       "12940  Что по статистике такие суммы сейчас вообще не...  ...         −\n",
       "18693  Но уже через 40 минут дзилинь и новая ссылка в...  ...         +\n",
       "\n",
       "[12871 rows x 3 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "dataset = pd.concat((dataset, double_2cat)).drop(columns=\"2category\").drop_duplicates()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "11o01fu0j94o4cwwwz3k72k",
    "execution_id": "d75476ea-788e-44f3-9b97-135b72be9db0"
   },
   "source": [
    "# Bert-like модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "cellId": "evmmkwwthoqj4i0f02sux"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_name = \"blanchefort/rubert-base-cased-sentiment-rusentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "cellId": "n388o8456uje06daktgo"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc03f22025e45279834466a42df35d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/495 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8e703b3d274722897436bca4462a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/952 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8135928e584375b8411ab7e7235cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bc14a80d6a4a03a3794360910d450e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c57198d02484ff0acdde4accf1fe14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "cellId": "vss4mmmqwynlp70fr0l0m"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class MultiTaskClassifier(nn.Module):\n",
    "    def __init__(self, in_features=768, n_classes=[3, 4], bias=True):\n",
    "        super(MultiTaskClassifier, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.classifiers = nn.ModuleList([nn.Linear(in_features, n_class, bias=bias) for n_class in n_classes])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return (classifier(x) for classifier in self.classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "cellId": "j4b6lyud3coz5mwywbc6ak"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model.classifier = MultiTaskClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "cellId": "uj1i9miordap4eljyvrwjk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  7289,  5979,  6193,   128,  1129,  1250,   322,   802,  3604,\n",
       "           4212,   340, 75879,   132,   102,     0,     0,     0,     0],\n",
       "         [  101, 29179,  3191,   130,  1248,  1094,  8238,   128,   301,   336,\n",
       "          23533,  4299,  1447,  8019,   130,  1094,  8238,   132,   102],\n",
       "         [  101,  4152,   128,  2498,   106,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " [101],\n",
       " [102],\n",
       " ['[CLS] Бардак полный, после этого и не хочется идти в сбербанк. [SEP] [PAD] [PAD] [PAD] [PAD]',\n",
       "  '[CLS] Моё мнение - работа очень быстрая, а с учётом первой такой операции - очень быстрая. [SEP]',\n",
       "  '[CLS] Привет, мир! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "CLS_TOKEN = [tokenizer.cls_token_id]\n",
    "SEP_TOKEN = [tokenizer.sep_token_id]\n",
    "samples = dataset.sample(2).sentence.values\n",
    "\n",
    "tokenized = tokenizer(\n",
    "    samples.tolist() + [\"Привет, мир!\"],\n",
    "    max_length=512,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "tokenized, CLS_TOKEN, SEP_TOKEN, tokenizer.batch_decode(tokenized.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "cellId": "fbtlzcrpltmlb1nbim9mk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Communication', '?', 'Quality', 'Price', 'Safety'], dtype=object)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "dataset[\"1category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "cellId": "uakj637oq7fpct5tdbgf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>При этом всегда получал качественные услуги.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>+</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>Не вижу, за что хотя бы 2 поставить, сервис на 1!</td>\n",
       "      <td>?</td>\n",
       "      <td>−</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>Вот так \"Мой любимый\" банк МКБ меня обманул.</td>\n",
       "      <td>?</td>\n",
       "      <td>−</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11640</th>\n",
       "      <td>Отвратительное отношение к клиентам.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>−</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>Всегда в любое время дня и ночи помогут, ответ...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>+</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19199</th>\n",
       "      <td>В очереди, кстати, узнал, что многие также зак...</td>\n",
       "      <td>Price</td>\n",
       "      <td>−</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12512</th>\n",
       "      <td>Мало того, что услуги навязали, так еще и вопр...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>−</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19056</th>\n",
       "      <td>Почему-то с этим банком постоянно возникают пр...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>−</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12940</th>\n",
       "      <td>Что по статистике такие суммы сейчас вообще не...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>−</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18693</th>\n",
       "      <td>Но уже через 40 минут дзилинь и новая ссылка в...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>+</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12871 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  ... target2\n",
       "4754        При этом всегда получал качественные услуги.  ...       0\n",
       "4417   Не вижу, за что хотя бы 2 поставить, сервис на 1!  ...      -1\n",
       "3629        Вот так \"Мой любимый\" банк МКБ меня обманул.  ...      -1\n",
       "11640               Отвратительное отношение к клиентам.  ...       0\n",
       "5571   Всегда в любое время дня и ночи помогут, ответ...  ...       0\n",
       "...                                                  ...  ...     ...\n",
       "19199  В очереди, кстати, узнал, что многие также зак...  ...       2\n",
       "12512  Мало того, что услуги навязали, так еще и вопр...  ...       1\n",
       "19056  Почему-то с этим банком постоянно возникают пр...  ...       0\n",
       "12940  Что по статистике такие суммы сейчас вообще не...  ...       0\n",
       "18693  Но уже через 40 минут дзилинь и новая ссылка в...  ...       0\n",
       "\n",
       "[12871 rows x 5 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "dataset['target1'] = dataset.sentiment.apply(lambda x: {\"−\":0, \"?\":1, \"+\":2}[x])\n",
    "dataset['target2'] = dataset[\"1category\"].apply(lambda x: int({\"Communication\":0, \"Quality\":1, \"Price\":2, \"Safety\":2, '?': -1}[x]))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "cellId": "avxe34gx8ojsybanfwpehf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, pre_tokenizing=False):\n",
    "        self.data = data\n",
    "\n",
    "        self.texts = data.sentence.tolist()\n",
    "        self.labels1 = data.target1.tolist()\n",
    "        self.labels2 = data.target2.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pre_tokenizing = pre_tokenizing\n",
    "        \n",
    "        if pre_tokenizing:\n",
    "            self.tokenized_texts = [self.tokenize(text) for text in tqdm(self.texts)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return tokenizer(\n",
    "            text,\n",
    "            max_length=512,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        label1 = self.labels1[item]\n",
    "        label2 = self.labels2[item]\n",
    "        if self.pre_tokenizing:\n",
    "            tokenized = self.tokenized_texts[item]\n",
    "        else:\n",
    "            tokenized = self.tokenize(self.texts[item])\n",
    "        return tokenized.input_ids.squeeze(dim=0), tokenized.attention_mask.squeeze(dim=0), torch.LongTensor([label1]), torch.LongTensor([label2])   \n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    tokens, attention_masks, labels1, labels2 = list(zip(*batch)) #, document_ids, sentences_ids = list(zip(*batch))\n",
    "    \n",
    "    tokens = pad_sequence(tokens, batch_first=True, padding_value=0)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels1 = torch.cat(labels1)\n",
    "    labels2 = torch.cat(labels2)\n",
    "#     labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return tokens, attention_masks, labels1, labels2 #, document_ids, sentences_ids  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "cellId": "63qu1y6i3mtai81wtv20p6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cd2076ca49403b8190776014698854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "bs = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TextDataset(\n",
    "        dataset[:128],\n",
    "        tokenizer,\n",
    "        pre_tokenizing=True,\n",
    "    ),\n",
    "    shuffle=True,\n",
    "    batch_size=bs,\n",
    "    collate_fn=my_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "cellId": "gw722e4ui4vlgby3ebtqdo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 44]), torch.Size([8, 44]), torch.Size([8]), torch.Size([8]))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "tokens, masks, labels1, labels2 = next(iter(train_loader))\n",
    "tokens.shape, masks.shape, labels1.shape, labels2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "cellId": "6k4fcbt5yvg80o3ed9tm2k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0186, -0.2031,  0.0031],\n",
       "         [ 0.1186,  0.4424,  0.5632],\n",
       "         [ 0.2765,  0.5597,  0.3002],\n",
       "         [-0.0345, -0.1821, -0.0096],\n",
       "         [-0.1083, -0.3894, -0.0736],\n",
       "         [-0.0284, -0.3166, -0.0480],\n",
       "         [-0.0178,  0.0606,  0.3956],\n",
       "         [ 0.0177, -0.1018,  0.0492]], grad_fn=<AddmmBackward>),\n",
       " tensor([[-0.1552,  0.2531,  0.6723, -0.1176],\n",
       "         [-0.1279,  0.0773,  0.0065,  0.3774],\n",
       "         [ 0.2090, -0.3031, -0.3114,  0.5013],\n",
       "         [-0.0987,  0.2325,  0.6246, -0.0036],\n",
       "         [-0.1642,  0.3122,  0.6606, -0.2224],\n",
       "         [-0.0750,  0.2882,  0.6345, -0.1894],\n",
       "         [-0.3039,  0.2397,  0.5655,  0.1618],\n",
       "         [-0.1057,  0.1758,  0.6512,  0.0637]], grad_fn=<AddmmBackward>)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "class BERTSentimentClassifier(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BERTSentimentClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        return list(self.model(input_ids, attention_mask=attention_mask).logits)\n",
    "\n",
    "model = BERTSentimentClassifier(model)\n",
    "model(tokens, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "cellId": "2y5qory6ntzwy4rhooz2t"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sent_train, sent_valid = train_test_split(dataset.sentence.unique(), train_size=0.8)\n",
    "sent_valid, sent_test = train_test_split(sent_valid, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "cellId": "ili0ywr5irs9abing9haqv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10307, 5), (1293, 5), (1271, 5))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "df_train = dataset[dataset.sentence.isin(sent_train)]\n",
    "df_valid = dataset[dataset.sentence.isin(sent_valid)]\n",
    "df_test = dataset[dataset.sentence.isin(sent_test)]\n",
    "\n",
    "df_train.shape, df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "cellId": "imyqrzadxfr7c6clghnje"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import gc\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class LightningBERT(pl.LightningModule):\n",
    "    def __init__(self, model, optim_params=dict(), batch_size=8,\n",
    "                 save_path=None, save_name=None, save_period=1000, check_all=False,\n",
    "                 loss_coef=1.0, pre_tokenizing=False, log_period=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.optim_params = optim_params\n",
    "        self.batch_size = batch_size\n",
    "        self.save_path = save_path\n",
    "        self.save_period = save_period\n",
    "        self.save_name = save_name\n",
    "        self.check_all = check_all\n",
    "        self.loss_coef = loss_coef\n",
    "        self.pre_tokenizing = pre_tokenizing\n",
    "        self.log_period = log_period\n",
    "\n",
    "        # make variables for storing true and pred labels from each batch\n",
    "        self.train_loss = 0\n",
    "        self.train_preds1 = []\n",
    "        self.train_preds2 = []\n",
    "        self.train_labels1 = []\n",
    "        self.train_labels2 = []\n",
    "        self.train_metric1 = []\n",
    "        self.train_metric2 = []\n",
    "        \n",
    "        self.valid_loss = 0\n",
    "        self.valid_preds1 = []\n",
    "        self.valid_preds2 = []\n",
    "        self.valid_labels1 = []\n",
    "        self.valid_labels2 = []\n",
    "        self.valid_metric1 = []\n",
    "        self.valid_metric2 = []\n",
    "        \n",
    "        self.test_loss = 0\n",
    "        self.test_preds1 = []\n",
    "        self.test_preds2 = []\n",
    "        self.test_labels1 = []\n",
    "        self.test_labels2 = []\n",
    "        self.test_metric1 = []\n",
    "        self.test_metric2 = []\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index =-1)\n",
    "        self.auroc1 = AUROC(num_classes=3)\n",
    "        self.auroc2 = AUROC(num_classes=4)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(filter(lambda param: param.requires_grad, self.model.parameters()), **self.optim_params)\n",
    "        return optimizer\n",
    "\n",
    "    def compute_loss(self, logits, labels):\n",
    "        return self.criterion(logits, labels)\n",
    "    \n",
    "    def compute_metrics1(self, preds, labels):\n",
    "        return self.auroc1(preds, labels)\n",
    "    \n",
    "    def compute_metrics2(self, preds, labels):\n",
    "        cond = labels != -1\n",
    "        if torch.sum(cond) == 0:\n",
    "            return 0\n",
    "        return self.auroc2(preds[cond], labels[cond])\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tokens, masks, labels1, labels2 = batch\n",
    "        \n",
    "        logits1, logits2 = self.model(tokens, masks)\n",
    "        loss = self.compute_loss(logits1, labels1) + self.loss_coef * self.compute_loss(logits2, labels2)\n",
    "\n",
    "        self.train_loss += loss.detach().cpu().numpy()\n",
    "        self.train_preds1.append(F.softmax(logits1, dim=-1).detach().cpu())\n",
    "        self.train_preds2.append(F.softmax(logits2, dim=-1).detach().cpu())\n",
    "        self.train_labels1.append(labels1.detach().cpu())\n",
    "        self.train_labels2.append(labels2.detach().cpu())\n",
    "        \n",
    "        self.train_metric1.append(self.compute_metrics1(self.train_preds1[-1], self.train_labels1[-1]))\n",
    "        self.train_metric2.append(self.compute_metrics2(self.train_preds2[-1], self.train_labels2[-1]))\n",
    "        \n",
    "        if (batch_idx + 1) % self.log_period == 0:\n",
    "            self.log(\"iter_train_loss:\", loss, prog_bar=True)\n",
    "            self.log(\"iter_train_metric1:\", np.mean(self.train_metric1), prog_bar=True)\n",
    "            self.log(\"iter_train_metric2:\", np.mean(self.train_metric2), prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, _):\n",
    "        self.train_loss /= self.train_len\n",
    "        metric1 = np.mean(self.train_metric1)\n",
    "        metric2 = np.mean(self.train_metric2)\n",
    "        \n",
    "        self.log(\"Train Loss:\", self.train_loss, prog_bar=True)\n",
    "        self.log(\"Train Metric 1:\", metric1, prog_bar=True)\n",
    "        self.log(\"Train Metric 2:\", metric2, prog_bar=True)\n",
    "        print(\"Train Loss: \", self.train_loss)\n",
    "        print(\"Train Metric 1:\", metric1)\n",
    "        print(\"Train Metric 2:\", metric2)\n",
    "\n",
    "        self.train_loss = 0\n",
    "        self.train_preds1 = []\n",
    "        self.train_preds2 = []\n",
    "        self.train_labels1 = []\n",
    "        self.train_labels2 = []\n",
    "        self.train_metric1 = []\n",
    "        self.train_metric2 = []\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        tokens, masks, labels1, labels2 = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits1, logits2 = self.model(tokens, masks)\n",
    "            loss = self.compute_loss(logits1, labels1) + self.loss_coef * self.compute_loss(logits2, labels2)\n",
    "\n",
    "        self.valid_loss += loss.detach().cpu().numpy()\n",
    "        self.valid_preds1.append(F.softmax(logits1, dim=-1).detach().cpu())\n",
    "        self.valid_preds2.append(F.softmax(logits2, dim=-1).detach().cpu())\n",
    "        self.valid_labels1.append(labels1.detach().cpu())\n",
    "        self.valid_labels2.append(labels2.detach().cpu())\n",
    "        \n",
    "        self.valid_metric1.append(self.compute_metrics1(self.valid_preds1[-1], self.valid_labels1[-1]))\n",
    "        self.valid_metric2.append(self.compute_metrics2(self.valid_preds2[-1], self.valid_labels2[-1]))\n",
    "        \n",
    "        if (batch_idx + 1) % self.log_period == 0:\n",
    "            self.log(\"iter_valid_loss:\", loss, prog_bar=True)\n",
    "            self.log(\"iter_valid_metric1:\", np.mean(self.valid_metric1), prog_bar=True)\n",
    "            self.log(\"iter_valid_metric2:\", np.mean(self.valid_metric2), prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, _):\n",
    "        self.valid_loss /= self.valid_len\n",
    "        metric1 = np.mean(self.valid_metric1)\n",
    "        metric2 = np.mean(self.valid_metric2)\n",
    "        \n",
    "        self.log(\"Valid Loss:\", self.valid_loss, prog_bar=True)\n",
    "        self.log(\"Valid Metric 1:\", metric1, prog_bar=True)\n",
    "        self.log(\"Valid Metric 2:\", metric2, prog_bar=True)\n",
    "        print(\"Valid Loss: \", self.valid_loss)\n",
    "        print(\"Valid Metric 1:\", metric1)\n",
    "        print(\"Valid Metric 2:\", metric2)\n",
    "\n",
    "        self.valid_loss = 0\n",
    "        self.valid_preds1 = []\n",
    "        self.valid_preds2 = []\n",
    "        self.valid_labels1 = []\n",
    "        self.valid_labels2 = []\n",
    "        self.valid_metric1 = []\n",
    "        self.valid_metric2 = []\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        tokens, masks, labels1, labels2 = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits1, logits2 = self.model(tokens, masks)\n",
    "            loss = self.compute_loss(logits1, labels1) + self.loss_coef * self.compute_loss(logits2, labels2)\n",
    "\n",
    "        self.test_loss += loss.detach().cpu().numpy()\n",
    "        self.test_preds1.append(F.softmax(logits1, dim=-1).detach().cpu())\n",
    "        self.test_preds2.append(F.softmax(logits2, dim=-1).detach().cpu())\n",
    "        self.test_labels1.append(labels1.detach().cpu())\n",
    "        self.test_labels2.append(labels2.detach().cpu())\n",
    "        \n",
    "        self.test_metric1.append(self.compute_metrics1(self.test_preds1[-1], self.test_labels1[-1]))\n",
    "        self.test_metric2.append(self.compute_metrics2(self.test_preds2[-1], self.test_labels2[-1]))\n",
    "        \n",
    "        if (batch_idx + 1) % self.log_period == 0:\n",
    "            self.log(\"iter_test_loss:\", loss, prog_bar=True)\n",
    "            self.log(\"iter_test_metric1:\", np.mean(self.test_metric1), prog_bar=True)\n",
    "            self.log(\"iter_test_metric2:\", np.mean(self.test_metric2), prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def test_epoch_end(self, _):\n",
    "        self.test_loss /= self.test_len\n",
    "        metric1 = np.mean(self.test_metric1)\n",
    "        metric2 = np.mean(self.test_metric2)\n",
    "        \n",
    "        self.log(\"Test Loss:\", self.test_loss, prog_bar=True)\n",
    "        self.log(\"Test Metric 1:\", metric1, prog_bar=True)\n",
    "        self.log(\"Test Metric 2:\", metric2, prog_bar=True)\n",
    "        print(\"Test Loss: \", self.test_loss)\n",
    "        print(\"Test Metric 1:\", metric1)\n",
    "        print(\"Test Metric 2:\", metric2)\n",
    "\n",
    "        self.test_loss = 0\n",
    "        self.test_preds1 = []\n",
    "        self.test_preds2 = []\n",
    "        self.test_labels1 = []\n",
    "        self.test_labels2 = []\n",
    "        self.test_metric1 = []\n",
    "        self.test_metric2 = []\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            TextDataset(\n",
    "                df_train[:128] if self.check_all else df_train,\n",
    "                tokenizer=tokenizer, pre_tokenizing=self.pre_tokenizing\n",
    "            ),\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=my_collate_fn,\n",
    "            num_workers=8,\n",
    "        )\n",
    "        \n",
    "        self.train_len = len(train_loader)\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_loader = DataLoader(\n",
    "            TextDataset(\n",
    "                df_valid[:128] if self.check_all else df_valid,\n",
    "                tokenizer=tokenizer, pre_tokenizing=self.pre_tokenizing\n",
    "            ),\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=my_collate_fn,\n",
    "            num_workers=8,\n",
    "        )\n",
    "        \n",
    "        self.valid_len = len(valid_loader)\n",
    "        return valid_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_loader = DataLoader(\n",
    "            TextDataset(\n",
    "                df_test[:128] if self.check_all else df_test,\n",
    "                tokenizer=tokenizer, pre_tokenizing=self.pre_tokenizing\n",
    "            ),\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=my_collate_fn,\n",
    "            num_workers=8,\n",
    "        )\n",
    "        \n",
    "        self.test_len = len(test_loader)\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "cellId": "q6yovou4izgurar5pxetmc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "cellId": "xvqtlvrol6fxhfeln1oz"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "cellId": "j4w03rv0a7t8e0n3u9wu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                    | Params\n",
      "------------------------------------------------------\n",
      "0 | model     | BERTSentimentClassifier | 177 M \n",
      "1 | criterion | CrossEntropyLoss        | 0     \n",
      "2 | auroc1    | AUROC                   | 0     \n",
      "3 | auroc2    | AUROC                   | 0     \n",
      "------------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.435   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d612a0e2f6a34dd5b76b0ddfe55b5b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30d6613a7d34ce3a0ad6aa6dd22657d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  0.04385051168041465\n",
      "Valid Metric 1: 0.67777777\n",
      "Valid Metric 2: 0.39583334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc46b67b2ce842efbbb80b66ff216b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0e90b8da0f445a8faffb9fe2424de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ba42e3e713493fb5d11a2c709fa45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  4.258743871141363\n",
      "Valid Metric 1: 0.8365373\n",
      "Valid Metric 2: 0.35923937\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16f5a43cab84d39a9bf06d1b7ce0a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  4.259929406790086\n",
      "Valid Metric 1: 0.82526207\n",
      "Valid Metric 2: 0.36715534\n",
      "Train Loss:  4.2930475919167135\n",
      "Train Metric 1: 0.81727296\n",
      "Train Metric 2: 0.38733885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937831d7d5f24931bf8b70c4a960587e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  4.492241615866438\n",
      "Valid Metric 1: 0.8171002\n",
      "Valid Metric 2: 0.35587922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6109570fd08d4424918a9ff567bdae13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  4.175564510218891\n",
      "Valid Metric 1: 0.8387909\n",
      "Valid Metric 2: 0.3717761\n",
      "Train Loss:  3.9029870087828535\n",
      "Train Metric 1: 0.8303795\n",
      "Train Metric 2: 0.41517207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047db30ade3b45b08cd56e35012e1e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  4.3325174907107415\n",
      "Valid Metric 1: 0.8305885\n",
      "Valid Metric 2: 0.3746295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e26b4db031f48559123883591e4a837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  4.369262061369272\n",
      "Valid Metric 1: 0.8335917\n",
      "Valid Metric 2: 0.35921302\n",
      "Train Loss:  3.6022724988758332\n",
      "Train Metric 1: 0.8403559\n",
      "Train Metric 2: 0.43378302\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "lightning_model = LightningBERT(\n",
    "    model=model,\n",
    "    optim_params={\"lr\":1e-4},\n",
    "    loss_coef=5.0,\n",
    "    batch_size=8,\n",
    "#     save_path=\"./checkpoints1\",\n",
    "#     save_name=\"model\",\n",
    "#     save_period=6000,\n",
    "#     check_all=True,\n",
    "    pre_tokenizing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=-1,\n",
    "    accelerator='gpu',\n",
    "    max_epochs=3,\n",
    "    gradient_clip_val=1.0,\n",
    "    val_check_interval=0.5,\n",
    "    accumulate_grad_batches=8,\n",
    ")\n",
    "\n",
    "trainer.fit(lightning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "cellId": "gs4kktpokkhec5vft5tw"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "cellId": "27e4b9akvmvxyrqzenrigm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                    | Params\n",
      "------------------------------------------------------\n",
      "0 | model     | BERTSentimentClassifier | 177 M \n",
      "1 | criterion | CrossEntropyLoss        | 0     \n",
      "2 | auroc1    | AUROC                   | 0     \n",
      "3 | auroc2    | AUROC                   | 0     \n",
      "------------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.435   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed07df4305004be3b26cd7b187b10442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9f06bdedbc4ee9be6ee3e014b1d5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  0.04724957619184329\n",
      "Valid Metric 1: 0.7222222\n",
      "Valid Metric 2: 0.39583334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb832bf62b44c0f8480cfe76c207743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d778372cd2da46df92fd3f0202e38319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbfe9fdedcc4efe87b75867013d25dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  4.798638603293601\n",
      "Valid Metric 1: 0.8408791\n",
      "Valid Metric 2: 0.3649226\n",
      "Train Loss:  2.595964371949966\n",
      "Train Metric 1: 0.87109536\n",
      "Train Metric 2: 0.47912076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5804d7d2f7174a63bb61670e2120a436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  4.700305378915351\n",
      "Valid Metric 1: 0.84101266\n",
      "Valid Metric 2: 0.36072624\n",
      "Train Loss:  2.5345465569208536\n",
      "Train Metric 1: 0.8630106\n",
      "Train Metric 2: 0.4773996152717814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514d5e566dd7478c90d8225a8f0b9c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  4.904612576611984\n",
      "Valid Metric 1: 0.8401528\n",
      "Valid Metric 2: 0.3638056\n",
      "Train Loss:  2.4965745442777387\n",
      "Train Metric 1: 0.86880916\n",
      "Train Metric 2: 0.47824976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6960789a534fd38ec3079ce58402c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  4.9607776131159\n",
      "Valid Metric 1: 0.840764\n",
      "Valid Metric 2: 0.36117175\n",
      "Train Loss:  2.4660740769746416\n",
      "Train Metric 1: 0.872556\n",
      "Train Metric 2: 0.48337486\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d15e630a42847ad95e650d0083e2cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:  5.076635917211756\n",
      "Valid Metric 1: 0.84075177\n",
      "Valid Metric 2: 0.363842\n",
      "Train Loss:  2.438610268792873\n",
      "Train Metric 1: 0.8698916\n",
      "Train Metric 2: 0.47480994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df97845839d649cfa5f1cf578af180c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1271 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512704f8dd3442bc823af56f1e444740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  5.017196777481703\n",
      "Test Metric 1: 0.81699914\n",
      "Test Metric 2: 0.40144348\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test Loss:': 5.0171966552734375,\n",
      " 'Test Metric 1:': 0.8169991374015808,\n",
      " 'Test Metric 2:': 0.4014434814453125,\n",
      " 'iter_test_loss:': 5.868220806121826,\n",
      " 'iter_test_metric1:': 0.8456400632858276,\n",
      " 'iter_test_metric2:': 0.4057683050632477}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'iter_test_loss:': 5.868220806121826,\n",
       "  'iter_test_metric1:': 0.8456400632858276,\n",
       "  'iter_test_metric2:': 0.4057683050632477,\n",
       "  'Test Loss:': 5.0171966552734375,\n",
       "  'Test Metric 1:': 0.8169991374015808,\n",
       "  'Test Metric 2:': 0.4014434814453125}]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "lightning_model = LightningBERT(\n",
    "    model=model,\n",
    "    optim_params={\"lr\":3e-6},\n",
    "    loss_coef=5.0,\n",
    "    batch_size=8,\n",
    "#     save_path=\"./checkpoints1\",\n",
    "#     save_name=\"model\",\n",
    "#     save_period=6000,\n",
    "#     check_all=True,\n",
    "    pre_tokenizing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=-1,\n",
    "    accelerator='gpu',\n",
    "    max_epochs=5,\n",
    "    val_check_interval=1.0,\n",
    "    accumulate_grad_batches=4,\n",
    ")\n",
    "\n",
    "trainer.fit(lightning_model)\n",
    "trainer.test(lightning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qhrpebm4dhs7rvb4z5l6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "0690cca5-5109-4d76-9879-d121c99834c1",
  "notebookPath": "SergeysentimentAnalysis.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
