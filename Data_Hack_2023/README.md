<div align="center">
  <br>
  <h1>HSE Data Science Hack 💻</h1>
  <strong>Хакатон от ФКН</strong>
</div>
<br

Ссылка на презентацию: https://www.canva.com/design/DAFfl_wxjkI/49vvT7CmAfm_dtQm7ZSCsg/edit

HSE Data Science Hack - это хакатон, организованный Факультетом компьютерных наук Национального исследовательского университета "Высшая школа экономики". Мероприятие пройдет 8-9 апреля и приглашает студентов бакалавриата, специалитета и магистратуры для участия в командах от 3 до 5 человек. Цель хакатона - создание инновационных проектов в области Data Science, а участникам предоставляется возможность работать с опытными экспертами в этой области и наладить контакты с представителями крупных IT-компаний.
    
Спасибо организаторам за этот интересный вызов и поздравляю всех, кто получил удовольствие от него!
    
## Problems

На хакатоне мы решали две задачи. Первой задачей является оценка (классификация) этической репутации банков на основе текстовой информации - отрывков из отзывов, которые оставили пользователи. Вторая задача была мультилейблинговой, и заключалась в том, чтобы определить какие, как минимум, две метки имеют отрывки из "Качество", "Цена", "Коммуникация", "Безопасность"

## Overview of Solution

![alt text](https://i.imgur.com/RaAR5pp.png)

### Duplicates

В данных для задачи классификации отзывов о банках были строки с одинаковыми комментариями, но которые были размечены по-разному. Мы использовали алгоритмы для удаления дубликатов из исходных данных. Это позволило нам очистить данные и уменьшить количество ошибок в нашей модели.
    
![alt text](https://i.imgur.com/Uz3dbsL.png)

### Tokenizing

Bag of Words является одним из самых простых и широко используемых подходов для анализа текстовых данных. Он заключается в том, чтобы представить каждый документ в виде мешка (bag) слов, где каждое слово является отдельным признаком, а количество его употреблений в тексте является значением этого признака.

Для использования BoW в вашей задаче, мы токенизировали каждый отзыв на отдельные слова, и затем создали мешок слов для каждого отзыва. Для создания мешка слов мы использовали словарь, содержащий все уникальные слова из всех отзывов. Каждый отзыв был представлен в виде вектора с количеством употреблений каждого слова из словаря в этом отзыве.
    
![alt text](https://miro.medium.com/v2/resize:fit:600/format:webp/0*JpqZhCNsQ_OGaRkB.jpg)

### Stemming
Стемминг (stemming) в области обработки естественного языка (Natural Language Processing, NLP) - это процесс приведения слова к его основе или корню (стему), путем удаления окончаний и суффиксов. Например, слова "бегу", "бегут", "бежать" будут приведены к общему стему "бег".

Стемминг используется для стандартизации слов и уменьшения числа уникальных слов, что упрощает процесс анализа текста. Это может быть полезно для поисковых систем, информационных систем, анализа тональности и других задач обработки текста.   

![alt text](https://qph.cf2.quoracdn.net/main-qimg-187b045c480fa7c0b16869daa0661b5a)

### Vectorizing

Во время обработки текстовых данных в рамках задачи классификации отзывов о банках мы использовали подход, основанный на подсчете вхождений корней слов. Для каждого текста подсчитали вектор, который содержал количество вхождений каждого корня слова.

Использование векторизации на основе корней слов позволило учитывать семантику слов, не зависящую от их формы, что повысило точность классификации. Кроме того, такой подход позволил уменьшить размерность признакового пространства и ускорить обучение модели
  
![alt text](https://i.imgur.com/a9DKauX.png)

    
### FCNN

В нашем случае, мы использовали FCNN с скрытым слоем и функцией активации ReLU. Для этого мы создали модель в PyTorch, которая содержала входной слой с размером, соответствующим размерности векторизованных текстов, скрытый слой с 256 нейронами и функцией активации ReLU и выходной слой с функцией активации ReLU, где каждый нейрон отвечал за вероятность соответствующего класса.

Для обучения FCNN мы использовали функцию потерь crossentropy и оптимизатор Adam.
